{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: sys\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import psycopg2\n",
    "from scipy.stats import ks_2samp\n",
    "import scipy.stats as scats\n",
    "import visuals as vs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from heapq import nlargest\n",
    "plt.style.use('ggplot') \n",
    "%matplotlib inlineimport sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import psycopg2\n",
    "from scipy.stats import ks_2samp\n",
    "import scipy.stats as scats\n",
    "import visuals as vs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.style.use('ggplot') \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC-III Critical Care Database\n",
    "\n",
    "MIMIC-III (Medical Information Mart for Intensive Care III) is a large, freely-available database comprising deidentified health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012.\n",
    "\n",
    "The database includes information such as demographics, vital sign measurements made at the bedside (~1 data point per hour), laboratory test results, procedures, medications, caregiver notes, imaging reports, and mortality (both in and out of hospital).\n",
    "\n",
    "MIMIC supports a diverse range of analytic studies spanning epidemiology, clinical decision-rule improvement, and electronic tool development. It is notable for three factors:\n",
    "\n",
    "it is freely available to researchers worldwide\n",
    "it encompasses a diverse and very large population of ICU patients\n",
    "it contains high temporal resolution data including lab results, electronic documentation, and bedside monitor trends and waveforms.\n",
    "\n",
    "Citations: \n",
    "MIMIC-III, a freely accessible critical care database. Johnson AEW, Pollard TJ, Shen L, Lehman L, Feng M, Ghassemi M, Moody B, Szolovits P, Celi LA, and Mark RG. Scientific Data (2016). DOI: 10.1038/sdata.2016.35. Available at: http://www.nature.com/articles/sdata201635\n",
    "\n",
    "Pollard, T. J. & Johnson, A. E. W. The MIMIC-III Clinical Database http://dx.doi.org/10.13026/C2XW26 (2016).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING DATA\n",
    "The mimic III database was downloaded and reconstructed locally using posgresql. The database was managed graphically using Portico. \n",
    "A query was run on the mimic III database to generate chart data for the first 24hrs of a patients icu stay as well as demographic data for patients diagnosed with sepsis according to the Angus criteria. \n",
    "(Angus et al, 2001. Epidemiology of severe sepsis in the United States; http://www.ncbi.nlm.nih.gov/pubmed/11445675 )\n",
    "\n",
    "The query was exported from Porticoto the file CHART_EVENTS_ANGUS_FIRST24.csv. The data was read into a pandas dataframe lab_events. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_csv('CHART_EVENTS_FIRST24.csv')\n",
    "data.head()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reorganizing the data\n",
    "The imported data uses subject_id as the index. The following code moves the subject_id data to a column, creates\n",
    "a proper index and reorganizes the columns to have the lab results grouped together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['subject_id'] = data.index\n",
    "data.set_index(np.arange(data.shape[0]), inplace = True)\n",
    "cols = list(data.columns)\n",
    "cols.insert(0, cols.pop(cols.index('icustay_id')))\n",
    "cols.insert(1, cols.pop(cols.index('subject_id')))\n",
    "data = data[cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2 = data.drop_duplicates('icustay_id', keep = 'first')\n",
    "data3 = data.drop_duplicates('subject_id', keep = 'first')\n",
    "print \"The number of unique ICU stays = {}\".format(data2.shape[0])\n",
    "print \"The number of unique patients  = {}\".format(data3.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display the different measurements captured in the database query\n",
    "labels = list(data.label.unique())\n",
    "display(data.shape)\n",
    "display(data[(data.label=='Capillary Refill') & (data.value.dropna())].head())\n",
    "display(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in labels:\n",
    "    print \"the number of samples for {} is {}\".format(item, data['icustay_id'][data.label == item].dropna().unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in labels:\n",
    "    icu_stays = data['icustay_id'][data.label == item].dropna().unique().shape[0]\n",
    "    if icu_stays > 2000:\n",
    "        print \"the number of samples for {} is {}\".format(item, icu_stays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_cols = [x for x in labels if (data['icustay_id'][data.label == x].dropna().unique().shape[0] >= 2000)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_cols_const = [old_cols[x] for x in [0, 12]]\n",
    "print old_cols_const\n",
    "old_cols_cat = [x for x in old_cols if 'GCS' in x]\n",
    "old_cols_cat.append(old_cols[6])\n",
    "print old_cols_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_cols2 = [x for x in old_cols if ((x not in old_cols_const) & (x not in old_cols_cat))]\n",
    "print old_cols2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_cols_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calc_dict_cols = ['pH', 'BP_Mean', 'BP_Dia', 'BP_Sys', 'pH2', 'HR', 'O2_Fraction', \n",
    "                   'RR', 'TempC', 'TempC_Calc']\n",
    "const_dict_cols = ['Weight', 'Height', 'O2_Fraction', 'RR', 'TempC', 'TempC_Calc']\n",
    "cat_dict_cols = ['GCS_Eye', 'GCS_Motor','GCS_Verbal', 'GCS_total', 'Cap_refill' ]\n",
    "\n",
    "\n",
    "mean_dict_new_cols = []\n",
    "med_dict_new_cols = []\n",
    "std_dict_new_cols = []\n",
    "skew_dict_new_cols = []\n",
    "min_dict_new_cols = []\n",
    "max_dict_new_cols = []\n",
    "for x in calc_dict_cols:\n",
    "    mean_dict_new_cols.append(x + '_mean')\n",
    "    med_dict_new_cols.append(x + '_med')\n",
    "    std_dict_new_cols.append(x + '_std')\n",
    "    skew_dict_new_cols.append(x + '_skew')\n",
    "    min_dict_new_cols.append(x + '_min')\n",
    "    max_dict_new_cols.append(x + '_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(old_cols2)):\n",
    "    print \"{}      {}\".format(old_cols2[i], max_dict_new_cols[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# height and weight are left out from the calculated measures because there was only one\n",
    "# measurement so they are constant.\n",
    "\n",
    "# IF I SWITCH THE ORDER OF THESE, I CAN MAKE THE OLD COLS KEYS FOR EACH DICT, THEN \n",
    "# I CAN LOOP THROUGH THE OLD COLS AND DO CALCULATIONS FOR EACH DICT RATHER THAN ITERATING THROUGH \n",
    "# THE COLUMNS FOR EACH \n",
    "mean_dict_names = dict(zip(mean_dict_new_cols, old_cols2))\n",
    "med_dict_names = dict(zip(med_dict_new_cols, old_cols2))\n",
    "std_dict_names = dict(zip(std_dict_new_cols, old_cols2))\n",
    "skew_dict_names = dict(zip(skew_dict_new_cols, old_cols2))\n",
    "min_dict_names = dict(zip(min_dict_new_cols, old_cols2))\n",
    "max_dict_names = dict(zip(max_dict_new_cols, old_cols2))\n",
    "const_dict_names = dict(zip(const_dict_cols, old_cols_const))\n",
    "cat_dict_names = dict(zip(cat_dict_cols, old_cols_cat))\n",
    "#display(mean_dict_names)\n",
    "#display(const_dict_names)\n",
    "#display(cat_dict_names)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "mean_dict_names = {'pH_1_mean_df':'Art.pH',\n",
    "                   'BP_mean2_df':'Arterial BP Mean',\n",
    "                   'BP_diastolic_mean_df':'Arterial BP [Diastolic]',  \n",
    "                   'BP_systolic_mean_df':'Arterial BP [Systolic]',\n",
    "                   'Height_mean_df': 'Height (cm)',\n",
    "                   'Weight_mean_df':'Admission Weight (Kg)', \n",
    "                   'pH_2_mean_df': 'Arterial pH', \n",
    "                   'HR_mean_df':'Heart Rate', \n",
    "                   'O2_fraction_mean_df':'Inspired O2 Fraction',\n",
    "                   'RR_mean_df':'Respiratory Rate', \n",
    "                   'Temp_mean_df':'Temperature C (calc)'\n",
    "                  }\n",
    "    \n",
    "med_dict_names = {'pH_1_med_df':'Art.pH',\n",
    "                   'BP_med_df':'Arterial BP Mean',\n",
    "                   'BP_diastolic_med_df':'Arterial BP [Diastolic]',  \n",
    "                   'BP_systolic_med_df':'Arterial BP [Systolic]',\n",
    "                   \n",
    "                   'pH_2_med_df': 'Arterial pH', \n",
    "                   'HR_mmed_df':'Heart Rate', \n",
    "                   'O2_fraction_med_df':'Inspired O2 Fraction',\n",
    "                   'RR_med_df':'Respiratory Rate', \n",
    "                   'Temp_med_df':'Temperature C (calc)'\n",
    "                  }\n",
    "\n",
    "\n",
    "std_dict_names = {'pH_1_std_df':'Art.pH',\n",
    "                   'BP_std_df':'Arterial BP Mean',\n",
    "                   'BP_diastolic_std_df':'Arterial BP [Diastolic]',  \n",
    "                   'BP_systolic_std_df':'Arterial BP [Systolic]',\n",
    "                   'pH_2_std_df': 'Arterial pH', \n",
    "                   'HR_std_df':'Heart Rate', \n",
    "                   'O2_fraction_std_df':'Inspired O2 Fraction',\n",
    "                   'RR_std_df':'Respiratory Rate', \n",
    "                   'Temp_std_df':'Temperature C (calc)'\n",
    "                  }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "skew_dict_names = {'pH_1_skew_df':'Art.pH',\n",
    "                   'BP_skew_df':'Arterial BP Mean',\n",
    "                   'BP_diastolic_skew_df':'Arterial BP [Diastolic]',  \n",
    "                   'BP_systolic_skew_df':'Arterial BP [Systolic]',\n",
    "                   'pH_2_skew_df': 'Arterial pH', \n",
    "                   'HR_skew_df':'Heart Rate', \n",
    "                   'O2_fraction_skew_df':'Inspired O2 Fraction',\n",
    "                   'RR_skew_df':'Respiratory Rate', \n",
    "                   'Temp_skew_df':'Temperature C (calc)'\n",
    "                  }\n",
    "\n",
    "\n",
    "min_dict_names = {'pH_1_min_df':'Art.pH',\n",
    "                   'BP_min_df':'Arterial BP Mean',\n",
    "                   'BP_diastolic_min_df':'Arterial BP [Diastolic]',  \n",
    "                   'BP_systolic_min_df':'Arterial BP [Systolic]',\n",
    "                   'pH_2_min_df': 'Arterial pH', \n",
    "                   'HR_min_df':'Heart Rate', \n",
    "                   'O2_fraction_min_df':'Inspired O2 Fraction',\n",
    "                   'RR_min_df':'Respiratory Rate', \n",
    "                   'Temp_min_df':'Temperature C (calc)'\n",
    "                  }\n",
    "\n",
    "\n",
    "max_dict_names = {'pH_1_max_df':'Art.pH',\n",
    "                   'BP_max_df':'Arterial BP Mean',\n",
    "                   'BP_diastolic_max_df':'Arterial BP [Diastolic]',  \n",
    "                   'BP_systolic_max_df':'Arterial BP [Systolic]',\n",
    "                   'pH_2_max_df': 'Arterial pH', \n",
    "                   'HR_max_df':'Heart Rate', \n",
    "                   'O2_fraction_max_df':'Inspired O2 Fraction',\n",
    "                   'RR_max_df':'Respiratory Rate', \n",
    "                   'Temp_max_df':'Temperature C (calc)'\n",
    "                  }\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "mean_dict = {}\n",
    "med_dict = {}\n",
    "std_dict = {}\n",
    "skew_dict = {}\n",
    "min_dict = {}\n",
    "max_dict = {}\n",
    "# ITERATING THROUGH THE VARIABLES, CALCULATING MEANS, MEDIANS, STD, SKEWNESS, MIN AND MAX'S FOR EACH ITERATION\n",
    "# COME BACK AND REFINE THIS SO THAT THE DATA COLUMN NAMES ARE THE DICTIONARY KEYS, THEN WE CAN JUST ITERATE \n",
    "# THROUGH THOSE AND DO CALCULATIONS FOR EACH DICT IN A SINGLE LOOP\n",
    "for col in mean_dict_names.keys():\n",
    "    mean_dict[col] = pd.DataFrame(data[data.label == mean_dict_names[col]].groupby('icustay_id')['valuenum'].mean())\n",
    "    mean_dict[col].columns = [mean_dict_names[col]]\n",
    "    mean_dict[col]['hospital_expired_flag'] = data[data.label == mean_dict_names[col]].groupby('icustay_id').hospital_expire_flag.first()\n",
    "    mean_dict[col]['gender'] = data[data.label == mean_dict_names[col]].groupby('icustay_id').gender.first()\n",
    "for col in med_dict_names.keys():\n",
    "    med_dict[col] = pd.DataFrame(data[data.label == med_dict_names[col]].groupby('icustay_id')['valuenum'].median())\n",
    "    med_dict[col].columns = [med_dict_names[col]]\n",
    "    med_dict[col]['hospital_expired_flag'] = data[data.label == med_dict_names[col]].groupby('icustay_id').hospital_expire_flag.first()\n",
    "    med_dict[col]['gender'] = data[data.label == med_dict_names[col]].groupby('icustay_id').gender.first()\n",
    "for col in std_dict_names.keys(): \n",
    "    std_dict[col] = pd.DataFrame(data[data.label == std_dict_names[col]].groupby('icustay_id')['valuenum'].std())\n",
    "    std_dict[col].columns = [std_dict_names[col]]\n",
    "    std_dict[col]['hospital_expired_flag'] = data[data.label == std_dict_names[col]].groupby('icustay_id').hospital_expire_flag.first()\n",
    "    std_dict[col]['gender'] = data[data.label == std_dict_names[col]].groupby('icustay_id').gender.first()\n",
    "for col in skew_dict_names.keys(): \n",
    "    skew_dict[col] = pd.DataFrame(data[data.label == skew_dict_names[col]].groupby('icustay_id')['valuenum'].skew())\n",
    "    skew_dict[col].columns = [skew_dict_names[col]]\n",
    "    skew_dict[col]['hospital_expired_flag'] = data[data.label == skew_dict_names[col]].groupby('icustay_id').hospital_expire_flag.first()\n",
    "    skew_dict[col]['gender'] = data[data.label == skew_dict_names[col]].groupby('icustay_id').gender.first()\n",
    "for col in min_dict_names.keys():   \n",
    "    min_dict[col] = pd.DataFrame(data[data.label == min_dict_names[col]].groupby('icustay_id')['valuenum'].min())\n",
    "    min_dict[col].columns = [min_dict_names[col]]\n",
    "    min_dict[col]['hospital_expired_flag'] = data[data.label == min_dict_names[col]].groupby('icustay_id').hospital_expire_flag.first()\n",
    "    min_dict[col]['gender'] = data[data.label == min_dict_names[col]].groupby('icustay_id').gender.first()\n",
    "for col in max_dict_names.keys():       \n",
    "    max_dict[col] = pd.DataFrame(data[data.label == max_dict_names[col]].groupby('icustay_id')['valuenum'].max())\n",
    "    max_dict[col].columns = [max_dict_names[col]]\n",
    "    max_dict[col]['hospital_expired_flag'] = data[data.label == max_dict_names[col]].groupby('icustay_id').hospital_expire_flag.first()\n",
    "    max_dict[col]['gender'] = data[data.label == max_dict_names[col]].groupby('icustay_id').gender.first()\n",
    "\n",
    "print \"Summary Calculations Complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_dict[col].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "const_dict = {}\n",
    "cat_dict = {}\n",
    "\n",
    "for col in const_dict_names.keys():\n",
    "    \n",
    "    dummy = data[data.label == const_dict_names[col]].groupby('icustay_id')\n",
    "    const_dict[col] = pd.DataFrame(dummy.valuenum.first())\n",
    "    const_dict[col].columns = [const_dict_names[col]]\n",
    "    const_dict[col]['hospital_expired_flag'] = dummy.hospital_expire_flag.first()\n",
    "    const_dict[col]['gender'] = dummy.gender.first()\n",
    "    \n",
    "    '''\n",
    "    const_dict[col] = pd.DataFrame(data[data.label == const_dict_names[col]].groupby('icustay_id')['valuenum'].first())\n",
    "    const_dict[col].columns = [const_dict_names[col]]\n",
    "    const_dict[col]['hospital_expired_flag'] = data.groupby('icustay_id').hospital_expire_flag.first()\n",
    "    const_dict[col]['gender'] = data.groupby('icustay_id').gender.first()\n",
    "    '''\n",
    "# GCS MEASURES DO HAVE CORRESPONDING VALUENUMS AS CATEGORIES. WILL NOT INCLUDE PRESENTLY\n",
    "for col in cat_dict_names.keys():\n",
    "    dummy = data[data.label == cat_dict_names[col]].groupby('icustay_id')\n",
    "    cat_dict[col] = pd.DataFrame(dummy.value.first()) \n",
    "    cat_dict[col].columns = [cat_dict_names[col]]\n",
    "    cat_dict[col]['hospital_expired_flag'] = dummy.hospital_expire_flag.first()\n",
    "    cat_dict[col]['gender'] = dummy.gender.first()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    cat_dict[col] = pd.DataFrame(data[data.label == cat_dict_names[col]].groupby('icustay_id')['value'].first())\n",
    "    cat_dict[col].columns = [cat_dict_names[col]]\n",
    "    cat_dict[col]['hospital_expired_flag'] = data.groupby('icustay_id').hospital_expire_flag.first()\n",
    "    cat_dict[col]['gender'] = data.groupby('icustay_id').gender.first()\n",
    "    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col = const_dict_names.keys()[0]\n",
    "const_dict[col][const_dict[col].gender == 'F'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to remove datapoints from normalized data with 3 or more variables that are outliers and plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#mean_dict[mean_dict.cols()[0]].head()\n",
    "# SKEW_DICT[COL] = FRAME OF SUBJECT_ID, WHATEVER VARIABLE IS BEING INDICATED BY COL, \n",
    "# SURVIVAL AND GENDER\n",
    "for col in skew_dict.keys():\n",
    "    # plot\n",
    "    # print col\n",
    "    dummy = skew_dict[col][skew_dict[col].gender == 'M'] \n",
    "   \n",
    "\n",
    "    '''\n",
    "    #display(dummy.head())\n",
    "    # MAY WANT TO DO THIS WITH SURVIVOR AND NON-SURVIVOR GROUPS \n",
    "\n",
    "    all_outliers = []\n",
    "    # TODO: Calculate Q1 (25th percentile of the data) for the given feature\n",
    "    Q1 = np.percentile(dummy[dummy.columns[0]].dropna(), 25)\n",
    "    #print Q1\n",
    "    # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n",
    "    Q3 = np.percentile(dummy[dummy.columns[0]].dropna(), 75)\n",
    "    #print Q3\n",
    "    # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "    step = 1.5*(Q3 - Q1)\n",
    "    # Display the outliers\n",
    "    # print \"Data points considered outliers for the feature '{}':\".format(feature)\n",
    "    # display(data2[~((data2[feature] >= Q1 - step) & (data2[feature] <= Q3 + step))])\n",
    "    # ~ == not\n",
    "    all_outliers.extend(dummy[~((dummy[dummy.columns[0]] >= Q1 - step) & \n",
    "                              (dummy[dummy.columns[0]] <= Q3 + step))].index)\n",
    "    # OPTIONAL: Select the indices for data points you wish to remove\n",
    "    print \"the total outlier indices = {}\".format(len(all_outliers))\n",
    "    \n",
    "    outliers  = all_outliers #list(outlier_df[outlier_df.counts 1>= 2].indices.values)\n",
    "    #print \"the following data points are outliers and will be removed: \\n{}\".format(outliers)\n",
    "    # Remove the outliers, if any were specified\n",
    "    #display(outliers[:5])\n",
    "    #boxcox_data2 = boxcox_data.drop(boxcox_data.index[outliers]).reset_index(drop = True)\n",
    "    dummy.drop(outliers, inplace = True)# .reset_index(drop = True)\n",
    "    \n",
    "    '''    \n",
    "    \n",
    "    plt.subplots(figsize=(10,4))\n",
    "    dummy[dummy.columns[0]][dummy.hospital_expired_flag==1].dropna().plot.kde(\n",
    "        alpha=1.0,label='Non-survival')\n",
    "    dummy[dummy.columns[0]][dummy.hospital_expired_flag==0].dropna().plot.kde(\n",
    "        alpha=1.0,label='Survival')\n",
    "\n",
    "    # add title, labels etc.\n",
    "    plt.title('{} measurement on ICU admission '.format(col) +\n",
    "               'vs ICU mortality \\n')\n",
    "    plt.xlabel(col)\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1),fontsize=12)\n",
    "    \n",
    "    \n",
    "    plt.xlim(dummy[dummy.columns[0]].dropna().quantile(0.001), dummy[dummy.columns[0]].dropna().quantile(0.999))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in cat_dict.keys():\n",
    "    col2 = cat_dict_names[col]\n",
    "    #print col\n",
    "    #print col2\n",
    "    vals = list(cat_dict[col][col2].unique())\n",
    "    #display(vals)\n",
    "    total = cat_dict[col].groupby(col2)[col2].count()\n",
    "   \n",
    "    dead = cat_dict[col][cat_dict[col].hospital_expired_flag == 1].groupby(col2)[col2].count()\n",
    "    dead.name = 'Survivors'\n",
    "    dead_percent = 100.00*(dead / total)\n",
    "    live = cat_dict[col][cat_dict[col].hospital_expired_flag == 0].groupby(col2)[col2].count()\n",
    "    live.name = 'Non_Survivors'\n",
    "    live_percent = 100.00*(live / total)\n",
    "    monkey = pd.concat([live_percent, dead_percent], axis = 1)\n",
    "\n",
    "    #display(monkey)\n",
    "\n",
    "    monkey.plot.bar(stacked = True, figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                alpha = 0.5, title = \"Survival Rate for \" + col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "const_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col = const_dict.keys()[0]\n",
    "col2 = const_dict_names[col]\n",
    "print \"{}   {}\".format(col, col2)\n",
    "\n",
    "dead = const_dict[col][(const_dict[col].hospital_expired_flag == 0)&\n",
    "                          (const_dict[col].gender == gend)&\n",
    "                          (const_dict[col][col2] >20)&\n",
    "                          (const_dict[col][col2] < 700)]\n",
    "display(dead[dead[col2] > 500])\n",
    "display(dead.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in const_dict.keys():\n",
    "    \n",
    "    col2 = const_dict_names[col]\n",
    "    vals = list(const_dict[col][col2].unique())\n",
    "    \n",
    "    gender = ['M', 'F'] \n",
    "    \n",
    "    for gend in gender:\n",
    "        \n",
    "        print gend\n",
    "        dead = const_dict[col][(const_dict[col].hospital_expired_flag == 1)&\n",
    "                              (const_dict[col].gender == gend)]\n",
    "                          #&(const_dict[col][col2] >20)]\n",
    "        dead.name = 'Non_Survivors'\n",
    "        live = const_dict[col][(const_dict[col].hospital_expired_flag == 0)&\n",
    "                              (const_dict[col].gender == gend)]\n",
    "                          #&(const_dict[col][col2] >20)]\n",
    "        live.name = 'Survivors'\n",
    "    \n",
    "    \n",
    "        #display(dummy.head())\n",
    "        # MAY WANT TO DO THIS WITH SURVIVOR AND NON-SURVIVOR GROUPS \n",
    "        '''\n",
    "        all_outliers = []\n",
    "        # TODO: Calculate Q1 (25th percentile of the data) for the given feature\n",
    "        Q1 = np.percentile(dummy[dummy.columns[0]].dropna(), 25)\n",
    "        #print Q1\n",
    "        # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n",
    "        Q3 = np.percentile(dummy[dummy.columns[0]].dropna(), 75)\n",
    "        #print Q3\n",
    "        # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "        step = 1.5*(Q3 - Q1)\n",
    "        # Display the outliers\n",
    "        # print \"Data points considered outliers for the feature '{}':\".format(feature)\n",
    "        # display(data2[~((data2[feature] >= Q1 - step) & (data2[feature] <= Q3 + step))])\n",
    "        # ~ == not\n",
    "        all_outliers.extend(dummy[~((dummy[dummy.columns[0]] >= Q1 - step) & \n",
    "                                  (dummy[dummy.columns[0]] <= Q3 + step))].index)\n",
    "        # OPTIONAL: Select the indices for data points you wish to remove\n",
    "        print \"the total outlier indices = {}\".format(len(all_outliers))\n",
    "    \n",
    "        outliers  = all_outliers #list(outlier_df[outlier_df.counts 1>= 2].indices.values)\n",
    "        #print \"the following data points are outliers and will be removed: \\n{}\".format(outliers)\n",
    "        # Remove the outliers, if any were specified\n",
    "        #display(outliers[:5])\n",
    "        #boxcox_data2 = boxcox_data.drop(boxcox_data.index[outliers]).reset_index(drop = True)\n",
    "        dummy.drop(outliers, inplace = True)# .reset_index(drop = True)    \n",
    "        '''   \n",
    "        maxx = 0.99\n",
    "        minn = 0.01\n",
    "    \n",
    "        live_max = live[col2].quantile(maxx)\n",
    "        live_min = live[col2].quantile(minn)\n",
    "        dead_max = dead[col2].quantile(maxx)\n",
    "        dead_min = dead[col2].quantile(minn)\n",
    "        maxlim = max(live_max, dead_max)\n",
    "        minlim = min(live_min, dead_min)\n",
    "   \n",
    "    \n",
    "    \n",
    "        plt.subplots(figsize=(10,4))\n",
    "        #live[(live[col2] < live_max) & (live[col2] > live_min)][col2].plot.hist(bins = 100, alpha=0.3,label='Survivors')\n",
    "        #live[(dead[col2] < dead_max) & (dead[col2] > dead_min)][col2].plot.hist(bins = 20, alpha=1.0,label='Non-Survivors')\n",
    "    \n",
    "        live[(live[col2] < live_max) & (live[col2] > live_min)][col2].plot.hist(bins = 100, \n",
    "                                                                            alpha=0.3,label='Survivors')\n",
    "    \n",
    "        dead[(dead[col2] < dead_max) & (dead[col2] > dead_min)][col2].plot.hist(bins = 100, \n",
    "                                                                            alpha=1.0,label='Non-Survivors')\n",
    "        # add title, labels etc.\n",
    "        plt.title('{} measurement on ICU admission'.format(col) + \n",
    "                   'vs ICU mortality by gender = {}\\n'.format(gend))\n",
    "        plt.xlabel(col)\n",
    "        plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1),fontsize=12)\n",
    "   \n",
    "    \n",
    "        print \"{}    {}\".format(maxlim, minlim)\n",
    "        plt.xlim(minlim, maxlim)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "live[col2].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(type(live[col2]))\n",
    "print live_max\n",
    "live[col2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = data.drop_duplicates('icustay_id', keep = 'first')\n",
    "data3 = data2.drop(['label', 'value', 'valuenum'], axis = 1)\n",
    "data3.set_index(['icustay_id'], inplace = True)\n",
    "\n",
    "\n",
    "for col in mean_dict.keys():\n",
    "    col2 = mean_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(mean_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "\n",
    "\n",
    "for col in med_dict.keys():\n",
    "    col2 = med_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(med_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "    \n",
    "for col in std_dict.keys():\n",
    "    col2 = std_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(std_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "\n",
    "for col in skew_dict.keys():\n",
    "    col2 = skew_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(skew_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "    \n",
    "for col in min_dict.keys():\n",
    "    col2 = min_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(min_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "    \n",
    "for col in max_dict.keys():\n",
    "    col2 = max_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(max_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "    \n",
    "for col in const_dict.keys():\n",
    "    col2 = const_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(const_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "    \n",
    "for col in cat_dict.keys():\n",
    "    col2 = cat_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(cat_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "\n",
    "data3['icustay_id'] = data3.index\n",
    "cols = list(data3.columns)\n",
    "cols.sort()\n",
    "cols.insert(0, cols.pop(cols.index('icustay_id')))\n",
    "cols.insert(1, cols.pop(cols.index('subject_id')))\n",
    "\n",
    "\n",
    "data3 = data3[cols]\n",
    "data3.set_index(np.arange(data3.shape[0]), inplace = True)\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data3.to_csv('CHART_EVENTS_FIRST24_PROCESSED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_b = data3.dropna()\n",
    "X_continuous = data4[list(data4.columns[data4.dtypes == 'float64'])]\n",
    "y = data4['hospital_expire_flag']\n",
    "display(X_continuous.shape)\n",
    "display(y.shape)\n",
    "\n",
    "below_zeros = list(X_continuous.columns[X_continuous.min() < 0])\n",
    "below_zeros\n",
    "\n",
    "for col in below_zeros:\n",
    "    col_min = X_continuous[col].min()\n",
    "    X_continuous[col] = X_continuous[col].apply(lambda x: x-col_min)\n",
    "\n",
    "    \n",
    "    \n",
    "X_continuous.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data4 = pd.DataFrame.from_csv('LAB_EVENTS_FIRST24_PROCESSED.csv')\n",
    "data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data3.set_index(['icustay_id'], inplace = True)\n",
    "data4.set_index(['icustay_id'], inplace = True)\n",
    "data4.drop('subject_id')\n",
    "\n",
    "# ADD CODE TO LABEL LAB VS CHART CHART TIME\n",
    "data3 = data3.merge(data4, left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True, suffixes = ('_chart', '_lab'))\n",
    "\n",
    "data3['icustay_id'] = data3.index\n",
    "cols = list(data3.columns)\n",
    "cols.sort()\n",
    "cols.insert(0, cols.pop(cols.index('icustay_id')))\n",
    "cols.insert(1, cols.pop(cols.index('subject_id')))\n",
    "cols.insert(2, cols.pop(cols.index('Height')))\n",
    "cols.insert(3, cols.pop(cols.index('Weight')))\n",
    "cols.insert(4, cols.pop(cols.index('charttime_x')))\n",
    "cols.insert(5, cols.pop(cols.index('charttime_y')))\n",
    "cols.insert(6, cols.pop(cols.index('intime_x')))\n",
    "cols.insert(7, cols.pop(cols.index('intime_y')))\n",
    "cols.insert(8, cols.pop(cols.index('outtime_x')))\n",
    "cols.insert(9, cols.pop(cols.index('outtime_y')))\n",
    "\n",
    "data3 = data3[cols]\n",
    "data3.set_index(np.arange(data3.shape[0]), inplace = True)\n",
    "\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data3.rename(columns = {'charttime_x':'charttime_chart', 'charttime_y':'charttime_lab', \n",
    "              'intime_x':'intime_chart', 'intime_y':'intime_lab', \n",
    "             'outtime_x':'outtime_chart', 'outtime_y':'outtime_lab'}, inplace = True)\n",
    "data3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data4 = data3.dropna(thresh = 57)\n",
    "data5 = data3.dropna(subset = ['BP_Dia_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in data5.columns:\n",
    "    print \"{} has  {} NaN values\".format(col,data5[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data5.dropna(thresh = 50).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in data4.columns:\n",
    "    print \"{} has  {} NaN values\".format(col,data4[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data3.drop(['admittime', 'intime', 'charttime', 'itemid', 'cgid', 'value', \n",
    "#            'valuenum', 'valueuom', 'label'], axis = 1, inplace = True)\n",
    "#data3.drop(['itemid', 'angus'], axis = 1, inplace = True) \n",
    "#            'valuenum', 'valueuom', 'label'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(data[(data.label=='WBC') & (data.valuenum.isnull())].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"CO        {}\".format(CO_df.shape)\n",
    "print \"WBC       {}\".format(WBC_df.shape)\n",
    "print \"RR        {}\".format(RR_df.shape)\n",
    "print \"MAP       {}\".format(MAP_df.shape)\n",
    "print \"HR        {}\".format(HR_df.shape)\n",
    "print \"HR2       {}\".format(HR2_df.shape)\n",
    "print \"CVP_med   {}\".format(CVP_med_df.shape)\n",
    "print \"CVP_min   {}\".format(CVP_min_df.shape)\n",
    "print \"pH        {}\".format(pH_df.shape)\n",
    "print \"Lac_mean  {}\".format(LAC_mean_df.shape)\n",
    "print \"Lac_min   {}\".format(LAC_min_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ptnt_demog = pd.DataFrame.from_csv('PTNT_DEMOG_ANGUS_rev.csv')\n",
    "ptnt_demog.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(data3.columns)\n",
    "display(ptnt_demog.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ptnt_demog2 = ptnt_demog[['gender', 'marital_status', 'ethnicity', 'insurance', 'first_careunit', \n",
    "                          'age', 'hospital_expire_flag']]\n",
    "ptnt_demog2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the patient demographic and summary chart data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data4 = data3.copy()\n",
    "data4 = data4.merge(ptnt_demog2, left_index = True, right_index = True, how='left', sort = True)\n",
    "data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data4.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df[np.isfinite(df['EPS'])]\n",
    "#data4[np.isfinite(data4['CO_max'])].shape\n",
    "#data5 = data4.drop(['CO_max', 'HR2_mean'], axis = 1)\n",
    "# DROP COLUMNS WHERE THERE ARE (ARBITRARILY) < 7600 VALUES\n",
    "data5 = data4[data4.columns[data4.isnull().sum() < 7600]]\n",
    "data5.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data5.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "float_vars = data5.dtypes[data5.dtypes == 'float'].index\n",
    "cat_vars = data5.dtypes[data5.dtypes == 'object'].index\n",
    "int_vars = data5.dtypes[data5.dtypes == 'int64'].index\n",
    "cat_vars = cat_vars.drop(['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#int_vars = int_vars.drop(['subject_id', 'hospital_expire_flag', 'angus'])\n",
    "#int_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HR2\n",
    "display(data5.dropna().shape)\n",
    "display(data5[data5.hospital_expire_flag == 1].dropna().shape)\n",
    "display(data5[data5.hospital_expire_flag == 0].dropna().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for thing in cat_vars[1:2]:\n",
    "    cats = list(data5[thing].unique())\n",
    "    print cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for thing in cat_vars[:2]:\n",
    "    datadict = dict()\n",
    "    cats = list(data5[thing].unique())\n",
    "    print thing\n",
    "    print cats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for thing in cat_vars:\n",
    "    datadict = dict()\n",
    "    cats = list(data5[thing].unique())\n",
    "    print thing\n",
    "    print cats\n",
    "\n",
    "    for cat in cats:\n",
    "        dead = data5[thing][(data5[thing] == cat) & (data5.hospital_expire_flag == 1)].dropna().count()\n",
    "        live = data5[thing][(data5[thing] == cat) & (data5.hospital_expire_flag == 0)].dropna().count()\n",
    "        total = float(live) + dead\n",
    "        datadict[cat] = (live/total, dead/total)\n",
    "\n",
    "    frame = pd.DataFrame.from_dict(datadict)\n",
    "    frame.index = ['Survivors', 'Non_Survivors']\n",
    "    \n",
    "    frame.transpose().plot.bar(stacked = True, figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                 alpha = 0.5, title = \"Percent Survival Rate for \" + thing)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(int_vars)\n",
    "int_vars[1:len(int_vars)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for thing in int_vars[1:len(int_vars)-1]:\n",
    "    datadict = dict()\n",
    "    cats = list(data5[thing].unique())\n",
    "    print thing\n",
    "    print cats\n",
    "\n",
    "    for cat in cats:\n",
    "        dead = data5[thing][(data5[thing] == cat) & (data5.hospital_expire_flag == 1)].dropna().count()\n",
    "        live = data5[thing][(data5[thing] == cat) & (data5.hospital_expire_flag == 0)].dropna().count()\n",
    "        #total = float(live) + dead\n",
    "        #datadict[cat] = (live/total, dead/total)\n",
    "        datadict[cat] = (live, dead)\n",
    "    frame = pd.DataFrame.from_dict(datadict)\n",
    "    frame.index = ['Survivors', 'Non_Survivors']\n",
    "    \n",
    "    frame.transpose().plot.bar(stacked = True, figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                 alpha = 0.5, title = \"Percent Survival Rate for \" + thing)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "float_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for thing in float_vars:\n",
    "    # plot\n",
    "    plt.subplots(figsize=(13,6))\n",
    "    data5[thing][data5.hospital_expire_flag==1].dropna().plot.kde(\n",
    "        alpha=1.0,label='Non-survival')\n",
    "    data5[thing][data5.hospital_expire_flag==0].dropna().plot.kde(\n",
    "        alpha=1.0,label='Survival')\n",
    "    \n",
    "  \n",
    "\n",
    "    # add title, labels etc.\n",
    "    plt.title('First {} measurement on ICU admission '.format(thing.lower()) +\n",
    "               'vs ICU mortality \\n')\n",
    "    plt.xlabel(thing)\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1),fontsize=12)\n",
    "   # plt.xlim(0, data5[thing].dropna().quantile(0.99))\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dead_men = data5.gender[(data5['gender']=='M') & (data5.hospital_expire_flag ==1)].dropna().count()\n",
    "dead_women = data5.gender[(data5['gender']=='F') & (data5.hospital_expire_flag ==1)].dropna().count()\n",
    "live_men = data5.gender[(data5['gender']=='M') & (data5.hospital_expire_flag ==0)].dropna().count()\n",
    "live_women = data5.gender[(data5['gender']=='F') & (data5.hospital_expire_flag ==0)].dropna().count()\n",
    "survival = pd.DataFrame([[live_men, live_women], \n",
    "                        [dead_men, dead_women]], \n",
    "                       columns = ['Men', 'Women'],\n",
    "                       index = ['Survivors', 'Non_Survivors'])\n",
    "survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "survival.transpose().plot.bar(stacked = True, figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                             alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.arange(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind = np.arange(1,3)    # the x locations for the groups\n",
    "width = 0.25       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "plt.figure(figsize= (20,10))\n",
    "p1 = plt.bar(ind - width/2, survival.iloc[0], width, color = 'green', edgecolor = 'black',\n",
    "            linewidth = 3)\n",
    "p2 = plt.bar(ind - width/2, survival.iloc[1], width, color = 'red', edgecolor = 'black', \n",
    "             linewidth = 3, bottom=survival.iloc[0])\n",
    "\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.title('Survivors vs. Non-Survivors')\n",
    "plt.xticks(ind, ('Men', 'Women'))\n",
    "#plt.yticks(np.arange(0, 81, 10))\n",
    "plt.legend((p1[0], p2[0]), ('Survivors', 'Non_Survivors'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ax = skew_df.plot.bar(figsize = (20,7), title = 'Skewness Values for Different Data Transforms')\n",
    "#ax.set_ylabel(\"Skewness\")\n",
    "#data5.groupby(['hospital_expire_flag'])[['gender']].count().plot(kind='bar', stacked = True )\n",
    "data5[cat_vars].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# a dictionary is created containing units of measure for the different lab measurements\n",
    "labs = {'ANIONGAP': 'mEq/L',\n",
    "        'ALBUMIN': 'g/dL',\n",
    "        'BICARBONATE': 'mEq/L',\n",
    "        'BILIRUBIN': 'mg/dL',\n",
    "        'BUN': '',\n",
    "        'CHLORIDE': 'mEq/L',\n",
    "        'CREATININE': 'mg/dL',\n",
    "        'GLUCOSE': 'mg/dL',\n",
    "        'HEMATOCRIT': '%',\n",
    "        'HEMOGLOBIN': 'g/dL',\n",
    "        'INR': '',\n",
    "        'LACTATE': 'mmol/L',\n",
    "        'MAGNESIUM': 'mmol/L',\n",
    "        'PHOSPHATE': 'mg/dL',\n",
    "        'PLATELET': 'K/uL',\n",
    "        'POTASSIUM': 'mEq/L',\n",
    "        'PT': '',\n",
    "        'PTT': 'sec',\n",
    "        'SODIUM':'mmol/L',\n",
    "        'WBC': ''}\n",
    "'''\n",
    "# a list is created containing units of measure for the lab measurements\n",
    "lab_units = ['g/dL',\n",
    "        'mEq/L',\n",
    "        'mEq/L',\n",
    "        'mg/dL',\n",
    "        '',\n",
    "        'mEq/L',\n",
    "        'mg/dL',\n",
    "        'mg/dL',\n",
    "        '%', \n",
    "        'g/dL',\n",
    "        '',\n",
    "        'mmol/L',\n",
    "        'mmol/L',\n",
    "        'mg/dL',\n",
    "        'K/uL',\n",
    "        'mEq/L',\n",
    "        '',\n",
    "        'sec',\n",
    "        'mmol/L',\n",
    "        '']\n",
    "\n",
    "# currently using lab_measures and lab_units to create the dict. \n",
    "# could be done more succinctly using the dictionary described above\n",
    "labs_dict = dict(zip(lab_measures, lab_units))\n",
    "print labs_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for l, u in iter(sorted(labs_dict.iteritems())):\n",
    "    # count patients\n",
    "    n_nonsurv = data[l][data.mort_icu==1].dropna().count()\n",
    "    n_surv = data[l][data.mort_icu==0].dropna().count()\n",
    "    \n",
    "    # get median, variance, skewness\n",
    "    med_nonsurv = data[l][data.mort_icu==1].dropna().median()\n",
    "    med_surv = data[l][data.mort_icu==0].dropna().median()\n",
    "    var_nonsurv = data[l][data.mort_icu==1].dropna().var()\n",
    "    var_surv = data[l][data.mort_icu==0].dropna().var()\n",
    "    skew_nonsurv = data[l][data.mort_icu==1].dropna().skew()\n",
    "    skew_surv = data[l][data.mort_icu==0].dropna().skew() \n",
    "    \n",
    "    # Are the 2 samples drawn from the same continuous distribution? \n",
    "    # Try Kolmogorov Smirnov test \n",
    "    ks_stat, p_val = ks_2samp(data[l][data.mort_icu==1].dropna(),\n",
    "                              data[l][data.mort_icu==0].dropna())\n",
    "\n",
    "    # plot\n",
    "    plt.subplots(figsize=(13,6))\n",
    "    data[l][data.mort_icu==1].dropna().plot.kde(\n",
    "        alpha=1.0,label='Non-survival (n={})'.format(n_nonsurv))\n",
    "    data[l][data.mort_icu==0].dropna().plot.kde(\n",
    "        alpha=1.0,label='Survival (n={})'.format(n_surv))\n",
    "    \n",
    "    # fake plots for KS test, median, etc\n",
    "    plt.plot([], label=' ',color='lightgray')\n",
    "    plt.plot([], label='KS test: p={}'.format(format(p_val,'.3f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Median (non-surv): {}'.format(format(med_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Median (surv): {}'.format(format(med_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Variance (non-surv): {}'.format(format(var_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Variance (surv): {}'.format(format(var_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Skew (non-surv): {}'.format(format(skew_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Skew (surv): {}'.format(format(skew_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "\n",
    "    # add title, labels etc.\n",
    "    plt.title('First {} measurement on ICU admission '.format(l.lower()) +\n",
    "               'vs ICU mortality \\n')\n",
    "    plt.xlabel(l + ' ' + u)\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1),fontsize=12)\n",
    "    plt.xlim(0, data[l].dropna().quantile(0.99))\n",
    "    \n",
    "    # Add lab range if available\n",
    "    if l in lab_ranges:\n",
    "        plt.axvline(lab_ranges[l][0],color='k',linestyle='--')\n",
    "        plt.axvline(lab_ranges[l][1],color='k',linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating box-plots for each variable for both survival and non-survival groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot first laboratory measurement on ICU admission vs ICU mortality\n",
    "# Would be clearer to plot CDF\n",
    "# Additional variables to be added: magnesium, phosphate, calcium\n",
    "\n",
    "labs = {'ANIONGAP': 'mEq/L',\n",
    "        'ALBUMIN': 'g/dL',\n",
    "        'BICARBONATE': 'mEq/L',\n",
    "        'BILIRUBIN': 'mg/dL',\n",
    "        'BUN': '',\n",
    "        'CHLORIDE': 'mEq/L',\n",
    "        'CREATININE': 'mg/dL',\n",
    "        'GLUCOSE': 'mg/dL',\n",
    "        'HEMATOCRIT': '%',\n",
    "        'HEMOGLOBIN': 'g/dL',\n",
    "        'INR': '',\n",
    "        'LACTATE': 'mmol/L',\n",
    "        'MAGNESIUM': 'mmol/L',\n",
    "        'PHOSPHATE': 'mg/dL',\n",
    "        'PLATELET': 'K/uL',\n",
    "        'POTASSIUM': 'mEq/L',\n",
    "        'PT': '',\n",
    "        'PTT': 'sec',\n",
    "        'SODIUM':'mmol/L',\n",
    "        'WBC': ''}\n",
    "\n",
    "for l, u in iter(sorted(labs.iteritems())):\n",
    "    # count patients\n",
    "    n_nonsurv = data[l.lower()+'_1st'][data.mort_icu==1].dropna().count()\n",
    "    n_surv = data[l.lower()+'_1st'][data.mort_icu==0].dropna().count()\n",
    "    \n",
    "    # get median, variance, skewness\n",
    "    med_nonsurv = data[l.lower()+'_1st'][data.mort_icu==1].dropna().median()\n",
    "    med_surv = data[l.lower()+'_1st'][data.mort_icu==0].dropna().median()\n",
    "    var_nonsurv = data[l.lower()+'_1st'][data.mort_icu==1].dropna().var()\n",
    "    var_surv = data[l.lower()+'_1st'][data.mort_icu==0].dropna().var()\n",
    "    skew_nonsurv = data[l.lower()+'_1st'][data.mort_icu==1].dropna().skew()\n",
    "    skew_surv = data[l.lower()+'_1st'][data.mort_icu==0].dropna().skew() \n",
    "    \n",
    "    # Are the 2 samples drawn from the same continuous distribution? \n",
    "    # Try Kolmogorov Smirnov test \n",
    "    ks_stat, p_val = ks_2samp(data[l.lower()+'_1st'][data.mort_icu==1].dropna(),\n",
    "                              data[l.lower()+'_1st'][data.mort_icu==0].dropna())\n",
    "\n",
    "    # plot\n",
    "    #fig, ax1 = plt.subplots(figsize=(13, 6))\n",
    "    #fig.canvas.set_window_title('A Boxplot Example')\n",
    "    #plt.subplots(figsize=(13,6))\n",
    "    #box_data = data[['mort_icu', l.lower()+'_1st']].dropna()\n",
    "    data[['mort_icu', l.lower()+'_1st']].dropna().boxplot(by='mort_icu', figsize = (13,5))\n",
    "    #data[['mort_icu', l.lower()+'_1st']].dropna().boxplot(by='mort_icu', figsize = (13,5))\n",
    "    #    label='Non-survival (n={})'.format(n_nonsurv))\n",
    "    plt.suptitle(\"\")\n",
    "    '''\n",
    "    data[l.lower()+'_1st'][data.mort_icu==0].dropna().plot.box(\n",
    "        label='Survival (n={})'.format(n_surv))\n",
    "    '''\n",
    "    \n",
    "    # fake plots for KS test, median, etc\n",
    "    plt.plot([], label=' ',color='lightgray')\n",
    "    plt.plot([], label='KS test: p={}'.format(format(p_val,'.3f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Median (non-surv): {}'.format(format(med_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Median (surv): {}'.format(format(med_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Variance (non-surv): {}'.format(format(var_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Variance (surv): {}'.format(format(var_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Skew (non-surv): {}'.format(format(skew_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Skew (surv): {}'.format(format(skew_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "    \n",
    "   # add title, labels etc.\n",
    "    plt.title('First {} measurement on ICU admission '.format(l.lower()) +\n",
    "               'vs ICU mortality \\n')\n",
    "    \n",
    "    plt.xlabel(l + ' ' + u)\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1),fontsize=12)\n",
    "   # plt.xlim(0, data[l.lower()+'_1st'].dropna().quantile(0.99))\n",
    "    \n",
    "    # Add lab range if available\n",
    "    if l in lab_ranges:\n",
    "        plt.axvline(lab_ranges[l][0],color='k',linestyle='--')\n",
    "        plt.axvline(lab_ranges[l][1],color='k',linestyle='--')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the density plots above, a list of variables was generated for which the distribution was very similar between survival and non-survival groups. (Maybe include similarity threshold??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate list of variables which, from density plots above, are similar between surivial and non-survival groups\n",
    "cols_list = list(data.columns)\n",
    "remove_list = ['chloride_1st', 'glucose_1st', 'hematocrit_1st', 'hemoglobin_1st', 'platelet_1st']\n",
    "for element in remove_list:\n",
    "    cols_list.remove(element)\n",
    "    \n",
    "display(cols_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data4 includes all variables except those identified as similar between survival and non-survival groups. \n",
    "data2 = data[cols_list]\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing data points that include any NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove samples which have any values that are nan\n",
    "data2 = data2.dropna()\n",
    "data2.set_index(np.arange(data2.shape[0]), inplace = True)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The survival rate calculated below indicates that 82% of patients survived. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dead = data2['mort_icu'][data2.mort_icu==1].count()\n",
    "survivors = data2['mort_icu'][data2.mort_icu==0].count()\n",
    "survival_rate = float(survivors)/(dead+survivors)\n",
    "print \"Number of  patients deceased = {}\".format(dead)\n",
    "print \"Number of patients           = {}\".format(dead + survivors)\n",
    "print \"Survival Rate                = {}\".format(survival_rate * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Classification on un-processed data with variables selected based on visual inspection\n",
    "The variables enumerated in 'remove_list' were observed for both survivors and non-survivors and appeared to be very similar in distribution. Because they appeared to be very similar for both groups, they were excluded as features for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data2[data2.columns[1:]], data2['mort_icu'], \n",
    "                                                    test_size = 0.30, random_state = 42)\n",
    "\n",
    "clf_SVC = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf', \n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False).fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(clf_SVC, data2[data2.columns[1:]], data2['mort_icu'], cv=5)\n",
    "display(scores)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_SVC.score(X_test, y_test)\n",
    "y_predsSVC = clf_SVC.predict(X_test)\n",
    "display(metrics.confusion_matrix(y_test, y_predsSVC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Confusion Matrix indicates that the classifier simply predicts survival for each patient which, because 82% of the patients survived, results in 82% Accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes Classification on un-processed data with variables selected based on visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data2[data2.columns[1:]], data2['mort_icu'], \n",
    "                                                    test_size = 0.40, random_state = 42)\n",
    "\n",
    "### create classifier\n",
    "clf_GNB = GaussianNB()\n",
    "### fit the classifier on the training features and labels\n",
    "clf_GNB.fit(X_train, y_train)\n",
    "    ### return the fit classifier\n",
    "\n",
    "nb_score = clf_GNB.score(X_test, y_test) \n",
    "\n",
    "print nb_score\n",
    "y_predsGNB = clf_GNB.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_predsGNB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Confusion Matrix indicates that the classifier predicts survival rates with ~78% accuracy with a mixture of predictions rather than predicting all survival as was the case for the SVM classifier. Unfortunately, the accuracy is not better than predicting all survivors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron Classification on un-processed data with variables selected based on visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data2[data2.columns[1:]], data2['mort_icu'], \n",
    "                                                    test_size = 0.20, random_state = 42)\n",
    "\n",
    "clf_MLP = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(7, 4), random_state=1)\n",
    "\n",
    "clf_MLP.fit(X_train, y_train)                         \n",
    "\n",
    "display(clf_MLP.score(X_test, y_test))\n",
    "y_predsMLP = clf_MLP.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_predsMLP)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataframe of skewness measurements for the raw data and for different transforms to calculate what transform does the best job of normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dict from which we'll build skew measures dataframe\n",
    "skew_dict2 = {data2.columns[1]:\n",
    "              {\n",
    "             'raw_skew': scats.skew(data2[data2.columns[1]]), \n",
    "             'sqrt_skew': scats.skew(np.sqrt(data2[data2.columns[1]])), \n",
    "             'log_skew': scats.skew(np.log(data2[data2.columns[1]].add(1))),  \n",
    "             'boxcox_skew': scats.skew(scats.boxcox(data2[data2.columns[1]].add(1))[0])\n",
    "             }\n",
    "             }\n",
    "\n",
    "print skew_dict2\n",
    "skew_df = pd.DataFrame.from_dict(skew_dict2, orient = 'index')  \n",
    "skew_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# re-order columns\n",
    "skew_df = skew_df[['raw_skew', 'sqrt_skew', 'log_skew', 'boxcox_skew']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skew_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "              \n",
    "for col in data2.columns[2:]:\n",
    "    raw_skew = scats.skew(data2[col])\n",
    "    \n",
    "    sqrt_skew = scats.skew(np.sqrt(data2[col]))\n",
    "    log_skew = scats.skew(np.log(data2[col].add(1)))\n",
    "        #print \"{} logskew = {}\".format(col, new_skew_val2)\n",
    "    boxcox_skew = scats.skew(scats.boxcox(data2[col].add(1))[0])\n",
    "    new_row = pd.Series([raw_skew, sqrt_skew, log_skew, boxcox_skew],\n",
    "                        #index=['measurement', 'raw_skew', 'sqrt_skew', 'log_skew', 'boxcox_skew'],\n",
    "                        index=['raw_skew', 'sqrt_skew', 'log_skew', 'boxcox_skew'],\n",
    "                       name = col)\n",
    "\n",
    "    skew_df = skew_df.append(new_row)#, ignore_index = True)\n",
    "\n",
    "skew_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skew_df.columns.name = 'Skewness Values'\n",
    "skew_df.index.name = 'Lab Measures'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The figure below shows the skewness values for each lab measurement for normalized data that has been transformed by taking the log, the square root and using the box-cox function. It can be seen that the box-cox function performed better than other transforms in reducing skewness. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = skew_df.plot.bar(figsize = (20,7), title = 'Skewness Values for Different Data Transforms')\n",
    "ax.set_ylabel(\"Skewness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sqrt_norm_data = np.sqrt(data2_norm[data2_norm.columns[1:]])\n",
    "#log_norm_data = np.log(data2_norm[data2_norm.columns[1:]].add(1))\n",
    "boxcox_data = data2.copy()\n",
    "for feature_name in data2.columns[1:]:\n",
    "    boxcox_data[feature_name] = scats.boxcox(data2[feature_name].add(1))[0]\n",
    "\n",
    "boxcox_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to identify outliers in normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_dict = {}\n",
    "suffix = '_outliers'\n",
    "\n",
    "\n",
    "for feature in boxcox_data.cols():\n",
    "    if feature != 'mort_icu':\n",
    "        # TODO: Calculate Q1 (25th percentile of the data) for the given feature\n",
    "        Q1 = np.percentile(boxcox_data[feature], 25)\n",
    "\n",
    "        # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n",
    "        Q3 = np.percentile(boxcox_data[feature], 75)\n",
    "\n",
    "        # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "        step = 1.5*(Q3 - Q1)\n",
    "\n",
    "        # Display the outliers\n",
    "        # print \"Data points considered outliers for the feature '{}':\".format(feature)\n",
    "        # display(data2[~((data2[feature] >= Q1 - step) & (data2[feature] <= Q3 + step))])\n",
    "        names_dict[feature+suffix] = boxcox_data[~((boxcox_data[feature] >= Q1 - step) & (boxcox_data[feature] <= Q3 + step))].index\n",
    "# OPTIONAL: Select the indices for data points you wish to remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to remove datapoints from normalized data with 3 or more variables that are outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_outliers = []\n",
    "for feature in names_dict.cols():\n",
    "    all_outliers.extend(names_dict[feature])\n",
    "print \"the total outlier indices = {}\".format(len(all_outliers))\n",
    "indices, counts = np.unique(all_outliers, return_counts = True)\n",
    "outlier_dict = {'counts': counts,\n",
    "                'indices': indices\n",
    "               }\n",
    "outlier_df = pd.DataFrame(outlier_dict)\n",
    "\n",
    "outliers  = list(outlier_df[outlier_df.counts >= 2].indices.values)\n",
    "print \"the following data points have >2 outlying feature and will be removed: \\n{}\".format(outliers)\n",
    "# Remove the outliers, if any were specified\n",
    "\n",
    "\n",
    "boxcox_data2 = boxcox_data.drop(boxcox_data.index[outliers]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# I believe this can be done more efficiently with sklearn.preprocessing.scale\n",
    "boxcox_data_scaled = boxcox_data2.copy()\n",
    "for feature_name in boxcox_data2.columns[1:]:\n",
    "#    max_value = data2[feature_name].max()\n",
    "#    min_value = data2[feature_name].min()\n",
    "#    data2_norm[feature_name] = (data2[feature_name] - min_value) / (max_value - min_value)\n",
    "    boxcox_data_scaled[feature_name] = preprocessing.scale(boxcox_data_scaled[feature_name], with_mean = True, \n",
    "                                                  with_std = True)\n",
    "    \n",
    "display(boxcox_data_scaled.head())\n",
    "display(boxcox_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# a dictionary is created containing units of measure for the different lab measurements\n",
    "labs_dict2 = labs_dict\n",
    "\n",
    "# could be done more succinctly using the dictionary described above\n",
    "\n",
    "for item in remove_list:\n",
    "    del labs_dict2[item]\n",
    "\n",
    "print labs_dict2\n",
    "print boxcox_data_scaled.columns\n",
    "\n",
    "import collections\n",
    "labs_dict3 = collections.OrderedDict(sorted(labs_dict2.items()))\n",
    "labs_dict3\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting frequency distribution for each lab measurement for both survival and non-survival groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for l, u in iter(sorted(labs_dict.iteritems())):\n",
    "    # count patients\n",
    "    n_nonsurv = boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==1].dropna().count()\n",
    "    n_surv = boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==0].dropna().count()\n",
    "    \n",
    "    # get median, variance, skewness\n",
    "    med_nonsurv = boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==1].dropna().median()\n",
    "    med_surv = boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==0].dropna().median()\n",
    "    var_nonsurv = boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==1].dropna().var()\n",
    "    var_surv = boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==0].dropna().var()\n",
    "    skew_nonsurv = boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==1].dropna().skew()\n",
    "    skew_surv = boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==0].dropna().skew() \n",
    "    \n",
    "    # Are the 2 samples drawn from the same continuous distribution? \n",
    "    # Try Kolmogorov Smirnov test \n",
    "    ks_stat, p_val = ks_2samp(boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==1].dropna(),\n",
    "                              boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==0].dropna())\n",
    "\n",
    "    # plot\n",
    "    plt.subplots(figsize=(13,6))\n",
    "    boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==1].dropna().plot.kde(\n",
    "        alpha=1.0,label='Non-survival (n={})'.format(n_nonsurv))\n",
    "    boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==0].dropna().plot.kde(\n",
    "        alpha=1.0,label='Survival (n={})'.format(n_surv))\n",
    "    \n",
    "    # fake plots for KS test, median, etc\n",
    "    plt.plot([], label=' ',color='lightgray')\n",
    "    plt.plot([], label='KS test: p={}'.format(format(p_val,'.3f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Median (non-surv): {}'.format(format(med_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Median (surv): {}'.format(format(med_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Variance (non-surv): {}'.format(format(var_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Variance (surv): {}'.format(format(var_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Skew (non-surv): {}'.format(format(skew_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Skew (surv): {}'.format(format(skew_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "\n",
    "    # add title, labels etc.\n",
    "    plt.title('First {} measurement on ICU admission '.format(l.lower()) +\n",
    "               'vs ICU mortality \\n')\n",
    "    plt.xlabel(l + ' ' + u)\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1),fontsize=12)\n",
    "    #plt.xlim(0, boxcox_data_scaled[l].dropna().quantile(0.99))\n",
    "    \n",
    "    # Add lab range if available\n",
    "    if l in lab_ranges:\n",
    "        plt.axvline(lab_ranges[l][0],color='k',linestyle='--')\n",
    "        plt.axvline(lab_ranges[l][1],color='k',linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing PCA on box-cox transformed, scaled data with outliers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# principle component analysis is used to reduce the dimensionality of data\n",
    "\n",
    "#pca = PCA(n_components = 4).fit(data2)\n",
    "pca = PCA(n_components = 4).fit(boxcox_data_scaled[boxcox_data_scaled.columns[1:]])\n",
    "\n",
    "# Generate PCA results plot\n",
    "pca_results = vs.pca_results(boxcox_data_scaled[boxcox_data_scaled.columns[1:]], pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduced_data = pca.transform(boxcox_data_scaled[boxcox_data_scaled.columns[1:]])\n",
    "# Create a DataFrame for the reduced data\n",
    "reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2', 'Dimension 3', 'Dimension 4']) #,\n",
    "                                                    #'Dimension 5', 'Dimension 6','Dimension 7', 'Dimension 8'])\n",
    "reduced_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot of Box-Cox transformed data with outliers removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.scatter_matrix(boxcox_data_scaled[1:], alpha = 0.3, figsize = (14,8), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized data principle components split into training and testing groups. A Gaussian Naive Bayes classifier is trained and tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training using 80% of data and testing using 20%. \n",
    "X_train, X_test, y_train, y_test = train_test_split(reduced_data, \n",
    "                                                    boxcox_data_scaled['mort_icu'], test_size = .30, random_state = 42)\n",
    "\n",
    "\n",
    "### create classifier\n",
    "clf_GNB = GaussianNB()\n",
    "### fit the classifier on the training features and labels\n",
    "clf_GNB.fit(X_train, y_train)\n",
    "    ### return the fit classifier\n",
    "\n",
    "nb_score = clf_GNB.score(X_test, y_test) \n",
    "\n",
    "print nb_score\n",
    "y_predsGNB = clf_GNB.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_predsGNB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box-Cox transformed data principle components split into training and testing groups. A Gaussian Naive Bayes classifier is trained and tested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classifier was trained and tested using principle components of normalized data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(boxcox_data_scaled[boxcox_data_scaled.columns[1:]], \n",
    "                                                    boxcox_data_scaled['mort_icu'], test_size = 0.40, random_state = 42)\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-6, learning_rate = 'adaptive',\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)                         \n",
    "\n",
    "display(clf.score(X_test, y_test))\n",
    "y_predsMLP = clf.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_predsMLP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[data.columns[5:]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classifier was trained and tested using principle components of normalized data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxcox_dead = boxcox_data_scaled[boxcox_data_scaled.mort_icu == 1]\n",
    "boxcox_survivors = boxcox_data_scaled[boxcox_data_scaled.mort_icu == 0]\n",
    "display(boxcox_dead.shape[0])\n",
    "display(boxcox_survivors.shape[0])\n",
    "boxcox_survivors_reduced = boxcox_survivors.sample(boxcox_dead.shape[0])\n",
    "frames = [boxcox_survivors_reduced, boxcox_dead]\n",
    "boxcox_even = pd.concat(frames)\n",
    "boxcox_even.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_samps = boxcox_even.shape[0]\n",
    "display(num_samps)\n",
    "\n",
    "boxcox_even2 = boxcox_even.sample(n=num_samps)\n",
    "#boxcox_even2.shape()\n",
    "boxcox_even2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxcox_even2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training using 80% of data and testing using 20%. \n",
    "X_train, X_test, y_train, y_test = train_test_split(boxcox_even2[boxcox_even2.columns[1:]], \n",
    "                                                    boxcox_even['mort_icu'], test_size = 0.40, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "### create classifier\n",
    "clf_GNB = GaussianNB()\n",
    "### fit the classifier on the training features and labels\n",
    "clf_GNB.fit(X_train, y_train)\n",
    "    ### return the fit classifier\n",
    "\n",
    "nb_score = clf_GNB.score(X_test, y_test) \n",
    "\n",
    "print nb_score\n",
    "y_predsGNB = clf_GNB.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_predsGNB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Classification on un-processed data with variables selected based on visual inspection\n",
    "The variables enumerated in 'remove_list' were observed for both survivors and non-survivors and appeared to be very similar in distribution. Because they appeared to be very similar for both groups, they were excluded as features for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(boxcox_even2[boxcox_even2.columns[1:]], \n",
    "                                                    boxcox_even['mort_icu'], test_size = 0.30, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "\n",
    "\n",
    "clf_SVC = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf', \n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False).fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(clf_SVC, boxcox_data_scaled[boxcox_data_scaled.columns[1:]], \n",
    "                         boxcox_data_scaled['mort_icu'], cv=5)\n",
    "display(scores)  \n",
    "y_predsSVC = clf_SVC.predict(X_test)\n",
    "display(metrics.confusion_matrix(y_test, y_predsSVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(clf_SVC.score(X_test, y_test))\n",
    "y_predsSVC = clf_SVC.predict(X_test)\n",
    "display(metrics.confusion_matrix(y_test, y_predsSVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(boxcox_even2[boxcox_even2.columns[1:]], \n",
    "                                                    boxcox_even['mort_icu'], test_size = 0.30, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-6, learning_rate = 'adaptive',\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)                         \n",
    "\n",
    "display(clf.score(X_test, y_test))\n",
    "y_predsMLP = clf.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_predsMLP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"moncol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diagnoses_codes = pd.DataFrame.from_csv('IDC9_DEADLY_DIAGNOSES.csv')\n",
    "diagnoses_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diagnoses_list = diagnoses_codes.icd9_code.unique()\n",
    "diagnoses_list2 = diagnoses_codes.diagnosis.unique()\n",
    "display(len(diagnoses_list))\n",
    "display(len(diagnoses_list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_codes = diagnoses_codes.drop_duplicates(['icd9_code', 'short_title'])\n",
    "unique_codes.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(diagnoses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diag_counts = diagnoses_codes.icd9_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diag_counts[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diagnoses_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxcox_data_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxcox_data_scaled_subset = boxcox_data_scaled[['mort_icu', 'albumin_1st', 'bicarbonate_1st', 'inr_1st', 'phosphate_1st',\n",
    "                                                'ptt_1st']]\n",
    "boxcox_data_scaled_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# principle component analysis is used to reduce the dimensionality of data\n",
    "\n",
    "#pca = PCA(n_components = 4).fit(data2)\n",
    "pca = PCA(n_components = 3).fit(boxcox_data_scaled[boxcox_data_scaled.columns[1:]])\n",
    "\n",
    "# Generate PCA results plot\n",
    "pca_results = vs.pca_results(boxcox_data_scaled[boxcox_data_scaled.columns[1:]], pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduced_data = pca.transform(boxcox_data_scaled[boxcox_data_scaled.columns[1:]])\n",
    "# Create a DataFrame for the reduced data\n",
    "reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2', 'Dimension 3', 'Dimension 4']) #,\n",
    "                                                    #'Dimension 5', 'Dimension 6','Dimension 7', 'Dimension 8'])\n",
    "reduced_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot of Box-Cox transformed data with outliers removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized data principle components split into training and testing groups. A Gaussian Naive Bayes classifier is trained and tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_list = list(boxcox_data_scaled_subset.columns[1:])\n",
    "feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training using 80% of data and testing using 20%. \n",
    "X_train, X_test, y_train, y_test = train_test_split(boxcox_data_scaled_subset[feat_list], \n",
    "                                                    boxcox_data_scaled_subset['mort_icu'], \n",
    "                                                    test_size = .30, random_state = 42)\n",
    "\n",
    "\n",
    "### create classifier\n",
    "clf_GNB = GaussianNB()\n",
    "### fit the classifier on the training features and labels\n",
    "clf_GNB.fit(X_train, y_train)\n",
    "    ### return the fit classifier\n",
    "\n",
    "nb_score = clf_GNB.score(X_test, y_test) \n",
    "\n",
    "print nb_score\n",
    "y_predsGNB = clf_GNB.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_predsGNB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
