{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import psycopg2\n",
    "from scipy.stats import ks_2samp\n",
    "import scipy.stats as scats\n",
    "import visuals as vs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from heapq import nlargest\n",
    "plt.style.use('ggplot') \n",
    "%matplotlib inline\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import Imputer\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import psycopg2\n",
    "from scipy.stats import ks_2samp\n",
    "import scipy.stats as scats\n",
    "import visuals as vs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.style.use('ggplot') \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC-III Critical Care Database\n",
    "\n",
    "MIMIC-III (Medical Information Mart for Intensive Care III) is a large, freely-available database comprising deidentified health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012.\n",
    "\n",
    "The database includes information such as demographics, vital sign measurements made at the bedside (~1 data point per hour), laboratory test results, procedures, medications, caregiver notes, imaging reports, and mortality (both in and out of hospital).\n",
    "\n",
    "MIMIC supports a diverse range of analytic studies spanning epidemiology, clinical decision-rule improvement, and electronic tool development. It is notable for three factors:\n",
    "\n",
    "it is freely available to researchers worldwide\n",
    "it encompasses a diverse and very large population of ICU patients\n",
    "it contains high temporal resolution data including lab results, electronic documentation, and bedside monitor trends and waveforms.\n",
    "\n",
    "Citations: \n",
    "MIMIC-III, a freely accessible critical care database. Johnson AEW, Pollard TJ, Shen L, Lehman L, Feng M, Ghassemi M, Moody B, Szolovits P, Celi LA, and Mark RG. Scientific Data (2016). DOI: 10.1038/sdata.2016.35. Available at: http://www.nature.com/articles/sdata201635\n",
    "\n",
    "Pollard, T. J. & Johnson, A. E. W. The MIMIC-III Clinical Database http://dx.doi.org/10.13026/C2XW26 (2016).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING DATA\n",
    "The mimic III database was downloaded and reconstructed locally using posgresql. The database was managed graphically using Portico. \n",
    "A query was run on the mimic III database to generate chart data for the first 24hrs of a patients icu stay as well as demographic data for patients diagnosed with sepsis according to the Angus criteria. \n",
    "(Angus et al, 2001. Epidemiology of severe sepsis in the United States; http://www.ncbi.nlm.nih.gov/pubmed/11445675 )\n",
    "\n",
    "The query was exported from Porticoto the file CHART_EVENTS_ANGUS_FIRST24.csv. The data was read into a pandas dataframe lab_events. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing chart data\n",
      "converting date-time data\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "print \"importing chart data\"\n",
    "data = pd.DataFrame.from_csv('CHART_EVENTS_FIRST24.csv')\n",
    "print \"converting date-time data\"\n",
    "data.loc[:,'charttime']  = pd.to_datetime(data.loc[:,'charttime'])\n",
    "data.head()  \n",
    "print \"complete\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reorganizing the data\n",
    "The imported data uses subject_id as the index. The following code moves the subject_id data to a column, creates\n",
    "a proper index and reorganizes the columns to have the lab results grouped together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>charttime</th>\n",
       "      <th>label</th>\n",
       "      <th>value</th>\n",
       "      <th>valuenum</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200001</td>\n",
       "      <td>55973</td>\n",
       "      <td>F</td>\n",
       "      <td>2181-11-26 16:55:00</td>\n",
       "      <td>GCS - Motor Response</td>\n",
       "      <td>Obeys Commands</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>55973</td>\n",
       "      <td>F</td>\n",
       "      <td>2181-11-26 14:00:00</td>\n",
       "      <td>Heart Rate</td>\n",
       "      <td>103</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200001</td>\n",
       "      <td>55973</td>\n",
       "      <td>F</td>\n",
       "      <td>2181-11-26 15:00:00</td>\n",
       "      <td>Respiratory Rate</td>\n",
       "      <td>30</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200001</td>\n",
       "      <td>55973</td>\n",
       "      <td>F</td>\n",
       "      <td>2181-11-26 15:00:00</td>\n",
       "      <td>Heart Rate</td>\n",
       "      <td>98</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200001</td>\n",
       "      <td>55973</td>\n",
       "      <td>F</td>\n",
       "      <td>2181-11-26 16:55:00</td>\n",
       "      <td>GCS - Verbal Response</td>\n",
       "      <td>Oriented</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   icustay_id  subject_id gender           charttime                  label  \\\n",
       "0      200001       55973      F 2181-11-26 16:55:00   GCS - Motor Response   \n",
       "1      200001       55973      F 2181-11-26 14:00:00             Heart Rate   \n",
       "2      200001       55973      F 2181-11-26 15:00:00       Respiratory Rate   \n",
       "3      200001       55973      F 2181-11-26 15:00:00             Heart Rate   \n",
       "4      200001       55973      F 2181-11-26 16:55:00  GCS - Verbal Response   \n",
       "\n",
       "            value  valuenum  hospital_expire_flag  \n",
       "0  Obeys Commands       6.0                     0  \n",
       "1             103     103.0                     0  \n",
       "2              30      30.0                     0  \n",
       "3              98      98.0                     0  \n",
       "4        Oriented       5.0                     0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['subject_id'] = data.index\n",
    "data.set_index(np.arange(data.shape[0]), inplace = True)\n",
    "cols = list(data.columns)\n",
    "cols.insert(0, cols.pop(cols.index('icustay_id')))\n",
    "cols.insert(1, cols.pop(cols.index('subject_id')))\n",
    "data = data[cols]\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique ICU stays = 60164\n",
      "The number of unique patients  = 45969\n"
     ]
    }
   ],
   "source": [
    "data2 = data.drop_duplicates('icustay_id', keep = 'first')\n",
    "data3 = data.drop_duplicates('subject_id', keep = 'first')\n",
    "print \"The number of unique ICU stays = {}\".format(data2.shape[0])\n",
    "print \"The number of unique patients  = {}\".format(data3.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6667722, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>charttime</th>\n",
       "      <th>label</th>\n",
       "      <th>value</th>\n",
       "      <th>valuenum</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>200017</td>\n",
       "      <td>15909</td>\n",
       "      <td>M</td>\n",
       "      <td>2138-03-18 04:00:00</td>\n",
       "      <td>Capillary Refill</td>\n",
       "      <td>Brisk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>200017</td>\n",
       "      <td>15909</td>\n",
       "      <td>M</td>\n",
       "      <td>2138-03-18 12:30:00</td>\n",
       "      <td>Capillary Refill</td>\n",
       "      <td>Brisk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>200017</td>\n",
       "      <td>15909</td>\n",
       "      <td>M</td>\n",
       "      <td>2138-03-17 23:00:00</td>\n",
       "      <td>Capillary Refill</td>\n",
       "      <td>Brisk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>200017</td>\n",
       "      <td>15909</td>\n",
       "      <td>M</td>\n",
       "      <td>2138-03-18 08:30:00</td>\n",
       "      <td>Capillary Refill</td>\n",
       "      <td>Brisk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>200017</td>\n",
       "      <td>15909</td>\n",
       "      <td>M</td>\n",
       "      <td>2138-03-18 20:30:00</td>\n",
       "      <td>Capillary Refill</td>\n",
       "      <td>Brisk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      icustay_id  subject_id gender           charttime             label  \\\n",
       "1183      200017       15909      M 2138-03-18 04:00:00  Capillary Refill   \n",
       "1185      200017       15909      M 2138-03-18 12:30:00  Capillary Refill   \n",
       "1187      200017       15909      M 2138-03-17 23:00:00  Capillary Refill   \n",
       "1189      200017       15909      M 2138-03-18 08:30:00  Capillary Refill   \n",
       "1191      200017       15909      M 2138-03-18 20:30:00  Capillary Refill   \n",
       "\n",
       "      value  valuenum  hospital_expire_flag  \n",
       "1183  Brisk       NaN                     0  \n",
       "1185  Brisk       NaN                     0  \n",
       "1187  Brisk       NaN                     0  \n",
       "1189  Brisk       NaN                     0  \n",
       "1191  Brisk       NaN                     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['GCS - Motor Response',\n",
       " 'Heart Rate',\n",
       " 'Respiratory Rate',\n",
       " 'GCS - Verbal Response',\n",
       " 'GCS - Eye Opening',\n",
       " 'Admission Weight (Kg)',\n",
       " 'Inspired O2 Fraction',\n",
       " 'Arterial BP [Diastolic]',\n",
       " 'Arterial pH',\n",
       " 'Temperature C (calc)',\n",
       " 'GCS Total',\n",
       " 'Creatinine (0-1.3)',\n",
       " 'Arterial BP [Systolic]',\n",
       " 'Glucose (70-105)',\n",
       " 'Arterial BP Mean',\n",
       " 'Hemoglobin',\n",
       " 'Art.pH',\n",
       " 'Hematocrit',\n",
       " 'Resp Rate (Total)',\n",
       " 'Resp Rate (Spont)',\n",
       " 'Temperature C',\n",
       " 'ART BP Diastolic',\n",
       " 'ART BP Systolic',\n",
       " 'Capillary Refill',\n",
       " 'Temperature Celsius',\n",
       " 'Height (cm)',\n",
       " 'Spont. Resp. Rate',\n",
       " 'pH (Art)',\n",
       " 'Arterial BP #2 [Diastolic]',\n",
       " 'Arterial BP Mean #2',\n",
       " 'Arterial BP #2 [Systolic]',\n",
       " 'ABP [Systolic]']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the different measurements captured in the database query\n",
    "labels = list(data.label.unique())\n",
    "display(data.shape)\n",
    "display(data[(data.label=='Capillary Refill') & (data.value.dropna())].head())\n",
    "display(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of samples for ABP [Systolic] is 1\n",
      "the number of samples for ART BP Diastolic is 1210\n",
      "the number of samples for ART BP Systolic is 1217\n",
      "the number of samples for Admission Weight (Kg) is 19950\n",
      "the number of samples for Art.pH is 16359\n",
      "the number of samples for Arterial BP #2 [Diastolic] is 333\n",
      "the number of samples for Arterial BP #2 [Systolic] is 336\n",
      "the number of samples for Arterial BP Mean is 15981\n",
      "the number of samples for Arterial BP Mean #2 is 333\n",
      "the number of samples for Arterial BP [Diastolic] is 15602\n",
      "the number of samples for Arterial BP [Systolic] is 16059\n",
      "the number of samples for Arterial pH is 17429\n",
      "the number of samples for Capillary Refill is 5927\n",
      "the number of samples for Creatinine (0-1.3) is 28909\n",
      "the number of samples for GCS - Eye Opening is 23303\n",
      "the number of samples for GCS - Motor Response is 23290\n",
      "the number of samples for GCS - Verbal Response is 23293\n",
      "the number of samples for GCS Total is 28907\n",
      "the number of samples for Glucose (70-105) is 28983\n",
      "the number of samples for Heart Rate is 60029\n",
      "the number of samples for Height (cm) is 8762\n",
      "the number of samples for Hematocrit is 28968\n",
      "the number of samples for Hemoglobin is 28428\n",
      "the number of samples for Inspired O2 Fraction is 11720\n",
      "the number of samples for Resp Rate (Spont) is 13122\n",
      "the number of samples for Resp Rate (Total) is 14087\n",
      "the number of samples for Respiratory Rate is 52231\n",
      "the number of samples for Spont. Resp. Rate is 18\n",
      "the number of samples for Temperature C is 5712\n",
      "the number of samples for Temperature C (calc) is 26051\n",
      "the number of samples for Temperature Celsius is 1784\n",
      "the number of samples for pH (Art) is 1073\n"
     ]
    }
   ],
   "source": [
    "labels.sort()\n",
    "\n",
    "for item in labels:\n",
    "    print \"the number of samples for {} is {}\".format(item, data['icustay_id'][data.label == item].dropna().unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Admission Weight (Kg)',\n",
       " 'Art.pH',\n",
       " 'Arterial BP Mean',\n",
       " 'Arterial BP [Diastolic]',\n",
       " 'Arterial BP [Systolic]',\n",
       " 'Arterial pH',\n",
       " 'Capillary Refill',\n",
       " 'Creatinine (0-1.3)',\n",
       " 'GCS - Eye Opening',\n",
       " 'GCS - Motor Response',\n",
       " 'GCS - Verbal Response',\n",
       " 'GCS Total',\n",
       " 'Glucose (70-105)',\n",
       " 'Heart Rate',\n",
       " 'Height (cm)',\n",
       " 'Hematocrit',\n",
       " 'Hemoglobin',\n",
       " 'Inspired O2 Fraction',\n",
       " 'Resp Rate (Spont)',\n",
       " 'Resp Rate (Total)',\n",
       " 'Respiratory Rate',\n",
       " 'Temperature C',\n",
       " 'Temperature C (calc)']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REMOVE ALL VARIABLES WITH FEWER THAN 2000 SAMPLES\n",
    "old_cols = [x for x in labels if (data['icustay_id'][data.label == x].dropna().unique().shape[0] >= 2000)]\n",
    "old_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Admission Weight (Kg)', 'Height (cm)']\n",
      "['GCS - Eye Opening', 'GCS - Motor Response', 'GCS - Verbal Response', 'GCS Total', 'Capillary Refill']\n"
     ]
    }
   ],
   "source": [
    "#CREATE LISTS FOR CONSTANT AND CATEGORICAL DATA\n",
    "\n",
    "#CONSTANT VARIABLES INCLUDE ADMISSION WEIGHT, HEIGHT\n",
    "old_cols_const = [old_cols[x] for x in [0, 14]]\n",
    "print old_cols_const\n",
    "#CATEGORICAL VARIABLES INCLUDE GLASGOW COMA SCALE (GSC)\n",
    "# AND CAPILLARY REFILL\n",
    "old_cols_cat = [x for x in old_cols if 'GCS' in x]\n",
    "old_cols_cat.append(old_cols[6])\n",
    "print old_cols_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Art.pH', 'Arterial BP Mean', 'Arterial BP [Diastolic]', 'Arterial BP [Systolic]', 'Arterial pH', 'Creatinine (0-1.3)', 'Glucose (70-105)', 'Heart Rate', 'Hematocrit', 'Hemoglobin', 'Inspired O2 Fraction', 'Resp Rate (Spont)', 'Resp Rate (Total)', 'Respiratory Rate', 'Temperature C', 'Temperature C (calc)']\n"
     ]
    }
   ],
   "source": [
    "old_cols2 = [x for x in old_cols if ((x not in old_cols_const) & (x not in old_cols_cat))]\n",
    "print old_cols2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING INDIVIDUAL FRAMES FOR MEASUREMENTS. \n",
    "WHAT IS UTILITY OF CREATING INDIVIDUAL FRAMES??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calc_dict_cols = ['pH2', 'BP_Mean', 'BP_Dia', 'BP_Sys', 'pH3', 'Creat2', 'GlucC', 'HR', 'Hemat','Hg', 'O2_Fraction', \n",
    "                   'RR_Spont','RR_Total', 'RR', 'TempC', 'TempC_Calc']\n",
    "const_dict_cols = ['Weight', 'Height', 'O2_Fraction', 'RR', 'TempC', 'TempC_Calc']\n",
    "cat_dict_cols = ['GCS_Eye', 'GCS_Motor','GCS_Verbal', 'GCS_total', 'Cap_refill' ]\n",
    "\n",
    "\n",
    "mean_dict_new_cols = []\n",
    "med_dict_new_cols = []\n",
    "std_dict_new_cols = []\n",
    "skew_dict_new_cols = []\n",
    "min_dict_new_cols = []\n",
    "max_dict_new_cols = []\n",
    "first_dict_new_cols = []\n",
    "slope_dict_new_cols = []\n",
    "delta_dict_new_cols = []\n",
    "for x in calc_dict_cols:\n",
    "    mean_dict_new_cols.append(x + '_mean')\n",
    "    med_dict_new_cols.append(x + '_med')\n",
    "    std_dict_new_cols.append(x + '_std')\n",
    "    skew_dict_new_cols.append(x + '_skew')\n",
    "    min_dict_new_cols.append(x + '_min')\n",
    "    max_dict_new_cols.append(x + '_max')\n",
    "    first_dict_new_cols.append(x + '_first')\n",
    "    slope_dict_new_cols.append(x + '_slope')\n",
    "    delta_dict_new_cols.append(x + '_delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Art.pH      pH2_max\n",
      "Arterial BP Mean      BP_Mean_max\n",
      "Arterial BP [Diastolic]      BP_Dia_max\n",
      "Arterial BP [Systolic]      BP_Sys_max\n",
      "Arterial pH      pH3_max\n",
      "Creatinine (0-1.3)      Creat2_max\n",
      "Glucose (70-105)      GlucC_max\n",
      "Heart Rate      HR_max\n",
      "Hematocrit      Hemat_max\n",
      "Hemoglobin      Hg_max\n",
      "Inspired O2 Fraction      O2_Fraction_max\n",
      "Resp Rate (Spont)      RR_Spont_max\n",
      "Resp Rate (Total)      RR_Total_max\n",
      "Respiratory Rate      RR_max\n",
      "Temperature C      TempC_max\n",
      "Temperature C (calc)      TempC_Calc_max\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(old_cols2)):\n",
    "    print \"{}      {}\".format(old_cols2[i], max_dict_new_cols[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_name creation complete\n"
     ]
    }
   ],
   "source": [
    "# height and weight are left out from the calculated measures because there was only one\n",
    "# measurement so they are constant.\n",
    "\n",
    "# IF I SWITCH THE ORDER OF THESE, I CAN MAKE THE OLD COLS KEYS FOR EACH DICT, THEN \n",
    "# I CAN LOOP THROUGH THE OLD COLS AND DO CALCULATIONS FOR EACH DICT RATHER THAN ITERATING THROUGH \n",
    "# THE COLUMNS FOR EACH \n",
    "mean_dict_names = dict(zip(mean_dict_new_cols, old_cols2))\n",
    "med_dict_names = dict(zip(med_dict_new_cols, old_cols2))\n",
    "std_dict_names = dict(zip(std_dict_new_cols, old_cols2))\n",
    "skew_dict_names = dict(zip(skew_dict_new_cols, old_cols2))\n",
    "min_dict_names = dict(zip(min_dict_new_cols, old_cols2))\n",
    "max_dict_names = dict(zip(max_dict_new_cols, old_cols2))\n",
    "first_dict_names = dict(zip(first_dict_new_cols, old_cols2))\n",
    "slope_dict_names = dict(zip(slope_dict_new_cols, old_cols2))\n",
    "delta_dict_names = dict(zip(delta_dict_new_cols, old_cols2))\n",
    "const_dict_names = dict(zip(const_dict_cols, old_cols_const))\n",
    "cat_dict_names = dict(zip(cat_dict_cols, old_cols_cat))\n",
    "#display(mean_dict_names)\n",
    "#display(const_dict_names)\n",
    "#display(cat_dict_names)\n",
    "\n",
    "print \"dict_name creation complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating mean values\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mean_dict = {}\n",
    "med_dict = {}\n",
    "std_dict = {}\n",
    "skew_dict = {}\n",
    "min_dict = {}\n",
    "max_dict = {}\n",
    "first_dict = {}\n",
    "slope_dict = {}\n",
    "delta_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ITERATING THROUGH THE VARIABLES, CALCULATING MEANS, MEDIANS, STD, SKEWNESS, MIN AND MAX'S FOR EACH ITERATION\n",
    "# COME BACK AND REFINE THIS SO THAT THE DATA COLUMN NAMES ARE THE DICTIONARY KEYS, THEN WE CAN JUST ITERATE \n",
    "# THROUGH THOSE AND DO CALCULATIONS FOR EACH DICT IN A SINGLE LOOP\n",
    "# ** CAN BE REPRESENTED MORE CONCISELY, SEE LABEVENTS_FIRST24.ipynb ** \n",
    "print \"calculating mean values\"\n",
    "for col in mean_dict_names.keys():\n",
    "    mean_dict[col] = pd.DataFrame(data[data.label == mean_dict_names[col]].groupby('icustay_id')['valuenum'].mean())\n",
    "    mean_dict[col].columns = [mean_dict_names[col]]\n",
    "    mean_dict[col]['hospital_expired_flag'] = data[data.label == mean_dict_names[col]].groupby('icustay_id').hospital_expire_flag.first()\n",
    "    mean_dict[col]['gender'] = data[data.label == mean_dict_names[col]].groupby('icustay_id').gender.first()\n",
    "print \"calculating med values\"\n",
    "for col in med_dict_names.keys():\n",
    "    med_dict[col] = pd.DataFrame(data[data.label == med_dict_names[col]].groupby('icustay_id')['valuenum'].median())\n",
    "    med_dict[col].columns = [med_dict_names[col]]\n",
    "    med_dict[col]['hospital_expired_flag'] = data[data.label == med_dict_names[col]].groupby('icustay_id').hospital_expire_flag.first()\n",
    "    med_dict[col]['gender'] = data[data.label == med_dict_names[col]].groupby('icustay_id').gender.first()\n",
    "print \"calculating std values\"\n",
    "for col in std_dict_names.keys(): \n",
    "    std_dict[col] = pd.DataFrame(data[data.label == std_dict_names[col]].groupby('icustay_id')['valuenum'].std())\n",
    "    std_dict[col].columns = [std_dict_names[col]]\n",
    "    std_dict[col]['hospital_expired_flag'] = data[data.label == std_dict_names[col]].groupby('icustay_id').hospital_expire_flag.first()\n",
    "    std_dict[col]['gender'] = data[data.label == std_dict_names[col]].groupby('icustay_id').gender.first()\n",
    "print \"calculating skewness values\"\n",
    "for col in skew_dict_names.keys(): \n",
    "    skew_dict[col] = pd.DataFrame(data[data.label == skew_dict_names[col]].groupby('icustay_id')['valuenum'].skew())\n",
    "    skew_dict[col].columns = [skew_dict_names[col]]\n",
    "    skew_dict[col]['hospital_expired_flag'] = data[data.label == skew_dict_names[col]].groupby('icustay_id').hospital_expire_flag.first()\n",
    "    skew_dict[col]['gender'] = data[data.label == skew_dict_names[col]].groupby('icustay_id').gender.first()\n",
    "print \"calculating min values\"\n",
    "for col in min_dict_names.keys():   \n",
    "    min_dict[col] = pd.DataFrame(data[data.label == min_dict_names[col]].groupby('icustay_id')['valuenum'].min())\n",
    "    min_dict[col].columns = [min_dict_names[col]]\n",
    "    min_dict[col]['hospital_expired_flag'] = data[data.label == min_dict_names[col]].groupby('icustay_id').hospital_expire_flag.first()\n",
    "    min_dict[col]['gender'] = data[data.label == min_dict_names[col]].groupby('icustay_id').gender.first()\n",
    "print \"calculating max values\"\n",
    "for col in max_dict_names.keys():       \n",
    "    max_dict[col] = pd.DataFrame(data[data.label == max_dict_names[col]].groupby('icustay_id')['valuenum'].max())\n",
    "    max_dict[col].columns = [max_dict_names[col]]\n",
    "    max_dict[col]['hospital_expired_flag'] = data[data.label == max_dict_names[col]].groupby('icustay_id').hospital_expire_flag.first()\n",
    "    max_dict[col]['gender'] = data[data.label == max_dict_names[col]].groupby('icustay_id').gender.first()\n",
    "\n",
    "print \"extracting first measurements\"\n",
    "for col in first_dict_names.keys():    \n",
    "    first_dict[col] = pd.DataFrame(data[data.label == first_dict_names[col]].groupby('icustay_id')['valuenum'].first())\n",
    "    first_dict[col].columns = [first_dict_names[col]]\n",
    "    first_dict[col]['hospital_expired_flag'] = data[data.label == first_dict_names[col]].groupby('icustay_id').hospital_expire_flag.first()\n",
    "    first_dict[col]['gender'] = data[data.label == first_dict_names[col]].groupby('icustay_id').gender.first()\n",
    "\n",
    "print \"calculating delta\"\n",
    "for col in delta_dict_names.keys():\n",
    "    delta_dict[col] = pd.DataFrame(data[data.label == delta_dict_names[col]].groupby('icustay_id')['valuenum'].last() - \n",
    "                                   data[data.label == delta_dict_names[col]].groupby('icustay_id')['valuenum'].first())\n",
    "    delta_dict[col].columns = [delta_dict_names[col]]\n",
    "    delta_dict[col]['hospital_expired_flag'] = data[data.label == delta_dict_names[col]].groupby('icustay_id').hospital_expire_flag.first()\n",
    "    delta_dict[col]['gender'] = data[data.label == delta_dict_names[col]].groupby('icustay_id').gender.first()\n",
    "\n",
    "print \"calculating slope\"\n",
    "for col in slope_dict_names.keys():\n",
    "    val_last = data[data.label == slope_dict_names[col]].groupby('icustay_id')['valuenum'].last()  \n",
    "    val_first = data[data.label == slope_dict_names[col]].groupby('icustay_id')['valuenum'].first()\n",
    "    time_last = data[data.label == slope_dict_names[col]].groupby('icustay_id')['charttime'].last()  \n",
    "    time_first = data[data.label == slope_dict_names[col]].groupby('icustay_id')['charttime'].first()\n",
    "    slope_dict[col] = pd.DataFrame((val_last - val_first)/((time_last - time_first)/np.timedelta64(1,'h')))  \n",
    "    slope_dict[col].columns = [slope_dict_names[col]]\n",
    "    slope_dict[col]['hospital_expired_flag'] = data[data.label == slope_dict_names[col]].groupby('icustay_id').hospital_expire_flag.first()\n",
    "    slope_dict[col]['gender'] = data[data.label == slope_dict_names[col]].groupby('icustay_id').gender.first()\n",
    "\n",
    "   \n",
    "print \"Summary Calculations Complete\"\n",
    "\n",
    "# CREATING FRAMES FOR CONSTANT AND CATEGORICAL DATA\n",
    "\n",
    "const_dict = {}\n",
    "cat_dict = {}\n",
    "\n",
    "for col in const_dict_names.keys():\n",
    "    \n",
    "    dummy = data[data.label == const_dict_names[col]].groupby('icustay_id')\n",
    "    const_dict[col] = pd.DataFrame(dummy.valuenum.first())\n",
    "    const_dict[col].columns = [const_dict_names[col]]\n",
    "    const_dict[col]['hospital_expired_flag'] = dummy.hospital_expire_flag.first()\n",
    "    const_dict[col]['gender'] = dummy.gender.first()\n",
    "    \n",
    "    '''\n",
    "    const_dict[col] = pd.DataFrame(data[data.label == const_dict_names[col]].groupby('icustay_id')['valuenum'].first())\n",
    "    const_dict[col].columns = [const_dict_names[col]]\n",
    "    const_dict[col]['hospital_expired_flag'] = data.groupby('icustay_id').hospital_expire_flag.first()\n",
    "    const_dict[col]['gender'] = data.groupby('icustay_id').gender.first()\n",
    "    '''\n",
    "# GCS MEASURES DO HAVE CORRESPONDING VALUENUMS AS CATEGORIES. WILL NOT INCLUDE PRESENTLY\n",
    "for col in cat_dict_names.keys():\n",
    "    dummy = data[data.label == cat_dict_names[col]].groupby('icustay_id')\n",
    "    cat_dict[col] = pd.DataFrame(dummy.value.first()) \n",
    "    cat_dict[col].columns = [cat_dict_names[col]]\n",
    "    cat_dict[col]['hospital_expired_flag'] = dummy.hospital_expire_flag.first()\n",
    "    cat_dict[col]['gender'] = dummy.gender.first()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    cat_dict[col] = pd.DataFrame(data[data.label == cat_dict_names[col]].groupby('icustay_id')['value'].first())\n",
    "    cat_dict[col].columns = [cat_dict_names[col]]\n",
    "    cat_dict[col]['hospital_expired_flag'] = data.groupby('icustay_id').hospital_expire_flag.first()\n",
    "    cat_dict[col]['gender'] = data.groupby('icustay_id').gender.first()\n",
    "    '''\n",
    "\n",
    "print \"Categorical Dataframes Complete\"\n",
    "\n",
    "calc_dicts = [mean_dict, med_dict, std_dict, skew_dict, min_dict, max_dict, first_dict, \n",
    "             slope_dict, delta_dict]\n",
    "\n",
    "names_dict = {}\n",
    "suffix = '_outliers'\n",
    "\n",
    "# SETTING OUTLIER DATA POINTS TO NAN FOR REMOVAL LATER USING DROPNA()\n",
    "for frame in calc_dicts:\n",
    "    for col in frame.keys():\n",
    "    # plot\n",
    "    # print col\n",
    "        dummy = frame[col]\n",
    "        col2 = dummy.columns[0]\n",
    "        #print \"{}   {}     {}\".format(col, col2, dummy.dropna().shape)\n",
    "        Q1 = np.percentile(dummy[col2].dropna(), 25)\n",
    "        # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n",
    "        Q3 = np.percentile(dummy[col2].dropna(), 75)\n",
    "        # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "        step = 1.5*(Q3 - Q1)\n",
    "        names_dict[col+suffix] = dummy[~((dummy[col2] >= Q1 - step) & (dummy[col2] <= Q3 + step))].index\n",
    "        dummy.set_value(names_dict[col+suffix], col2, np.NaN)\n",
    "        #print \"{}   {}     {}\".format(col, col2, dummy.dropna().shape)\n",
    "\n",
    "\n",
    "\n",
    "print \"Outlier Removal Complete\"\n",
    "print \"Complete\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to Plot Density of Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_dict['TempC_mean'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame = 'TempC_mean'\n",
    "col = 'Temperature C'\n",
    "\n",
    "dummy = mean_dict[frame]\n",
    "x25 = dummy[col].dropna().quantile(0.25)\n",
    "x50 = dummy[col].dropna().quantile(0.50)\n",
    "x75 = dummy[col].dropna().quantile(0.75)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplots(figsize=(13,6))\n",
    "\n",
    "dummy[dummy.hospital_expired_flag==1][col].dropna().plot.kde(\n",
    "        alpha=1.0,label='Non-survival')\n",
    "dummy[dummy.hospital_expired_flag==0][col].dropna().plot.kde(\n",
    "        alpha=1.0,label='Survival')\n",
    "\n",
    "plt.axvline(x = x25, color='k', linestyle='-')\n",
    "plt.text(x25+0.05,0.05,'Q1',rotation=0)\n",
    "plt.axvline(x = x50, color = 'k', linestyle = '-')\n",
    "plt.text(x50+0.05,0.05,'Q2',rotation=0)\n",
    "plt.axvline(x = x75, color = 'k', linestyle = '-')\n",
    "plt.text(x75+0.05,0.05,'Q3 ',rotation=0)\n",
    "\n",
    "#plt.plot((x25, x25), (y1, y2), 'k-')\n",
    "        # add title, labels etc.\n",
    "#plt.title('{} measurement on ICU admission '.format(col) +\n",
    "#                'vs ICU mortality \\n')\n",
    "plt.title('Mean Temperature Measurement Distributions for Survivors and Non-Survivors ')\n",
    "plt.xlabel(col)\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1),fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_dict['BP_Sys_mean']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# REMOVE FRAMES/VARIABLES FOR WHICH THERE IS ONLY ONE VALUE I.E. SINGULAR\n",
    "\n",
    "for frame in calc_dicts:\n",
    "    for col in frame.keys():\n",
    "        col2 = frame[col].columns[0]\n",
    "        unique_vals = len(frame[col][col2].dropna().unique())\n",
    "        if unique_vals < 2:\n",
    "            print \"removing due to only one value  = {}\".format(col)\n",
    "            frame.pop(col) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "frame = calc_dicts[0]\n",
    "for col in frame.keys():\n",
    "    dummy = frame[col]\n",
    "    col2 = dummy.columns[0]\n",
    "    \n",
    "    # REMOVE FRAMES / VARIABLES THAT ARE SINGULAR I.E. ONLY ONE VALUE. \n",
    "    '''\n",
    "    if np.linalg.matrix_rank(frame[col][col2]) < 2:\n",
    "        frame.pop(col)\n",
    "    else:\n",
    "    '''\n",
    "   \n",
    "    plt.subplots(figsize=(10,4))\n",
    "    dummy[col2][dummy.hospital_expired_flag==1].dropna().plot.kde(\n",
    "        alpha=1.0,label='Non-survival')\n",
    "    dummy[col2][dummy.hospital_expired_flag==0].dropna().plot.kde(\n",
    "        alpha=1.0,label='Survival')\n",
    "\n",
    "        # add title, labels etc.\n",
    "    plt.title('{} measurement on ICU admission '.format(col) +\n",
    "                'vs ICU mortality \\n')\n",
    "    plt.xlabel(col)\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1),fontsize=12)\n",
    "    \n",
    "print \"complete\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in cat_dict.keys():\n",
    "    col2 = cat_dict_names[col]\n",
    "    #print col\n",
    "    #print col2\n",
    "    vals = list(cat_dict[col][col2].unique())\n",
    "    #display(vals)\n",
    "    total = cat_dict[col].groupby(col2)[col2].count()\n",
    "   \n",
    "    dead = cat_dict[col][cat_dict[col].hospital_expired_flag == 1].groupby(col2)[col2].count()\n",
    "    dead.name = 'Survivors'\n",
    "    dead_percent = 100.00*(dead / total)\n",
    "    live = cat_dict[col][cat_dict[col].hospital_expired_flag == 0].groupby(col2)[col2].count()\n",
    "    live.name = 'Non_Survivors'\n",
    "    live_percent = 100.00*(live / total)\n",
    "    monkey = pd.concat([live_percent, dead_percent], axis = 1)\n",
    "\n",
    "    #display(monkey)\n",
    "\n",
    "    monkey.plot.bar(stacked = True, figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                alpha = 0.5, title = \"Survival Rate for \" + col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PLOTTING CONSTANT VALUES LIKE HEIGHT AND WEIGHT\n",
    "\n",
    "for col in const_dict.keys():\n",
    "    \n",
    "    col2 = const_dict_names[col]\n",
    "    vals = list(const_dict[col][col2].unique())\n",
    "    \n",
    "    gender = ['M', 'F'] \n",
    "    \n",
    "    for gend in gender:\n",
    "        \n",
    "        print gend\n",
    "        dead = const_dict[col][(const_dict[col].hospital_expired_flag == 1)&\n",
    "                              (const_dict[col].gender == gend)]\n",
    "                          #&(const_dict[col][col2] >20)]\n",
    "        dead.name = 'Non_Survivors'\n",
    "        live = const_dict[col][(const_dict[col].hospital_expired_flag == 0)&\n",
    "                              (const_dict[col].gender == gend)]\n",
    "                          #&(const_dict[col][col2] >20)]\n",
    "        live.name = 'Survivors'\n",
    "    \n",
    "    \n",
    "        #display(dummy.head())\n",
    "        # MAY WANT TO DO THIS WITH SURVIVOR AND NON-SURVIVOR GROUPS \n",
    "        '''\n",
    "        all_outliers = []\n",
    "        # TODO: Calculate Q1 (25th percentile of the data) for the given feature\n",
    "        Q1 = np.percentile(dummy[dummy.columns[0]].dropna(), 25)\n",
    "        #print Q1\n",
    "        # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n",
    "        Q3 = np.percentile(dummy[dummy.columns[0]].dropna(), 75)\n",
    "        #print Q3\n",
    "        # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "        step = 1.5*(Q3 - Q1)\n",
    "        # Display the outliers\n",
    "        # print \"Data points considered outliers for the feature '{}':\".format(feature)\n",
    "        # display(data2[~((data2[feature] >= Q1 - step) & (data2[feature] <= Q3 + step))])\n",
    "        # ~ == not\n",
    "        all_outliers.extend(dummy[~((dummy[dummy.columns[0]] >= Q1 - step) & \n",
    "                                  (dummy[dummy.columns[0]] <= Q3 + step))].index)\n",
    "        # OPTIONAL: Select the indices for data points you wish to remove\n",
    "        print \"the total outlier indices = {}\".format(len(all_outliers))\n",
    "    \n",
    "        outliers  = all_outliers #list(outlier_df[outlier_df.counts 1>= 2].indices.values)\n",
    "        #print \"the following data points are outliers and will be removed: \\n{}\".format(outliers)\n",
    "        # Remove the outliers, if any were specified\n",
    "        #display(outliers[:5])\n",
    "        #boxcox_data2 = boxcox_data.drop(boxcox_data.index[outliers]).reset_index(drop = True)\n",
    "        dummy.drop(outliers, inplace = True)# .reset_index(drop = True)    \n",
    "        '''   \n",
    "        maxx = 0.99\n",
    "        minn = 0.01\n",
    "    \n",
    "        live_max = live[col2].quantile(maxx)\n",
    "        live_min = live[col2].quantile(minn)\n",
    "        dead_max = dead[col2].quantile(maxx)\n",
    "        dead_min = dead[col2].quantile(minn)\n",
    "        maxlim = max(live_max, dead_max)\n",
    "        minlim = min(live_min, dead_min)\n",
    "   \n",
    "    \n",
    "    \n",
    "        plt.subplots(figsize=(10,4))\n",
    "        #live[(live[col2] < live_max) & (live[col2] > live_min)][col2].plot.hist(bins = 100, alpha=0.3,label='Survivors')\n",
    "        #live[(dead[col2] < dead_max) & (dead[col2] > dead_min)][col2].plot.hist(bins = 20, alpha=1.0,label='Non-Survivors')\n",
    "    \n",
    "        live[(live[col2] < live_max) & (live[col2] > live_min)][col2].plot.hist(bins = 100, \n",
    "                                                                            alpha=0.3,label='Survivors')\n",
    "    \n",
    "        dead[(dead[col2] < dead_max) & (dead[col2] > dead_min)][col2].plot.hist(bins = 100, \n",
    "                                                                            alpha=1.0,label='Non-Survivors')\n",
    "        # add title, labels etc.\n",
    "        plt.title('{} measurement on ICU admission'.format(col) + \n",
    "                   'vs ICU mortality by gender = {}\\n'.format(gend))\n",
    "        plt.xlabel(col)\n",
    "        plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1),fontsize=12)\n",
    "   \n",
    "    \n",
    "        print \"{}    {}\".format(maxlim, minlim)\n",
    "        plt.xlim(minlim, maxlim)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "## MERGING CONTINUOUS DATA INCLUDING CONSTANTS (WEIGHT, HEIGHT ETC) INTO SINGLE DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MERGE DATAFRAMES HERE \n",
    "# QUESTION UTILITY OF HAVING INDIVIDUAL FRAMES\n",
    "# ** CAN BE CODED MORE EFFICIENTLY. SEE LABEVENTS_FIRST24.ipynb ** \n",
    "data2 = data.drop_duplicates('icustay_id', keep = 'first')\n",
    "data3 = data2.drop(['label', 'value', 'valuenum'], axis = 1)\n",
    "data3.set_index(['icustay_id'], inplace = True)\n",
    "\n",
    "\n",
    "print \"Merging Mean Values\"\n",
    "for col in mean_dict.keys():\n",
    "    col2 = mean_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(mean_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "\n",
    "print \"Merging Median Values\"\n",
    "for col in med_dict.keys():\n",
    "    col2 = med_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(med_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "print \"Merging STD Values\"  \n",
    "for col in std_dict.keys():\n",
    "    col2 = std_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(std_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "print \"Merging Skew Values\"\n",
    "for col in skew_dict.keys():\n",
    "    col2 = skew_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(skew_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "\n",
    "print \"Merging Min Values\"\n",
    "for col in min_dict.keys():\n",
    "    col2 = min_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(min_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "    \n",
    "print \"Merging Max Values\"\n",
    "for col in max_dict.keys():\n",
    "    col2 = max_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(max_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "    \n",
    "print \"Merging First Values\"\n",
    "for col in first_dict.keys():\n",
    "    col2 = first_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(first_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "\n",
    "print \"Merging Delta Values\"\n",
    "for col in delta_dict.keys():\n",
    "    col2 = delta_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(delta_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "    \n",
    "print \"Merging Slope Values\"\n",
    "for col in slope_dict.keys():\n",
    "    col2 = slope_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(slope_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "    \n",
    "print \"Merging Constant Values\"\n",
    "for col in const_dict.keys():\n",
    "    col2 = const_dict_names[col]\n",
    "    data3 = data3.merge(pd.DataFrame(const_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "\n",
    "\n",
    "'''    \n",
    "CREATING SEPARATE DATAFRAMES FOR CATEGORICAL DATA \n",
    "\n",
    "for col in cat_dict.keys():\n",
    "    col2 = cat_dict_names[col]\n",
    "    data3 = data3.merge(pd.get_dummies(cat_dict[col][col2]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "'''\n",
    "\n",
    "data3['icustay_id'] = data3.index\n",
    "cols = list(data3.columns)\n",
    "cols.sort()\n",
    "cols.insert(0, cols.pop(cols.index('icustay_id')))\n",
    "cols.insert(1, cols.pop(cols.index('subject_id')))\n",
    "cols.insert(2, cols.pop(cols.index('hospital_expire_flag')))\n",
    "\n",
    "\n",
    "data3 = data3[cols]\n",
    "data3.set_index(np.arange(data3.shape[0]), inplace = True)\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONVERTING CATEGORICAL TO DUMMY DATA AND BREAKING UP INTO AFFINITY GROUPS. THREE RESULTING DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummies = data3[data3.columns[:3]]\n",
    "dummies.set_index(['icustay_id'], inplace = True)\n",
    "#display(monkey.head())\n",
    "for col in cat_dict.keys():\n",
    "    col2 = cat_dict_names[col]\n",
    "    chimp = pd.get_dummies(cat_dict[col][col2], prefix = cat_dict[col][col2].name)\n",
    "    dummies = dummies.merge(chimp, left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    \n",
    "    '''\n",
    "    newcols = list(data3.columns)\n",
    "    newcols.pop()\n",
    "    newcols.append(col)\n",
    "    data3.columns = newcols\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#monkey = monkey.merge(chimp, left_index = True, right_index = True, \n",
    "#                       how = 'left', sort = True)\n",
    "\n",
    "display(dummies.head())\n",
    "\n",
    "# MEASURES HAVE LOW AFFINITY I.E. WHEN WE DROP NAN VALUES THERE ARE VERY FEW SAMPLES LEFT \n",
    "# SO BREAKING THESE UP INTO HIGH AFFINITY DATAFRAMES FOR PROCESSING. \n",
    "# ** MAY CONSIDER PROCESSING GCS_TOTAL AS A CONTINUOUS BUT, FOR NOW CREATING DUMMIES\n",
    "\n",
    "print \"Shape of Capillary Block\"\n",
    "# NUMBER OF NON NAN SAMPLES IN CAPILLARY REFILL\n",
    "display(dummies[[x for x in dummies.columns if 'Capillary' in x]].dropna().shape)\n",
    "\n",
    "# NUMBER OF NON NAN SAMPLES IN GCS_TOTAL ONLY\n",
    "print \"Shape of GCS_Total Block\"\n",
    "display(dummies[[x for x in dummies.columns if 'Total' in x]].dropna().shape)\n",
    "# NUMBER OF NON NAN SAMPLES IN GCS MEASURES WITHOUT TOTAL \n",
    "print \"Shape of GCS Block\"\n",
    "display(dummies[[x for x in dummies.columns if (('GCS' in x) & ('Total' not in x))]].dropna().shape)\n",
    "# NUMBER OF NON NAN SAMPLES IN GCS TOTAL AND MEASEURES\n",
    "print \"Shape of All GCS  Block\"\n",
    "display(dummies[[x for x in dummies.columns if 'GCS' in x]].dropna().shape)\n",
    "# NUMBER OF NON NAN SAMPLES IN GCS MEASURES AND CAP REFILL\n",
    "print \"Shape of Capillary and GCS Block\"\n",
    "display(dummies[[x for x in dummies.columns if 'Total' not in x]].dropna().shape)\n",
    "\n",
    "#CREATE 3 BLOCKS BASED ON AFFINITY I.E. SIZE AFTER NAN VALUES DROPPED\n",
    "cap_cols = [x for x in dummies.columns if 'Capillary' in x]\n",
    "cap_cols.insert(0, 'hospital_expire_flag')\n",
    "Cap_dummies = dummies[cap_cols].dropna()\n",
    "GCS_Tot_cols = [x for x in dummies.columns if 'Total' in x]\n",
    "GCS_Tot_cols.insert(0, 'hospital_expire_flag')\n",
    "GCS_Total_dummies = dummies[GCS_Tot_cols].dropna()\n",
    "GCS_cols = [x for x in dummies.columns if (('GCS' in x) & ('Total' not in x))]\n",
    "GCS_cols.insert(0, 'hospital_expire_flag')\n",
    "GCS_dummies = dummies[GCS_cols].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROPPING CAPILLARY FILLING BLOCK AS THEY HAVE FEW DATA POINTS (~5K VS 20-30K FOR GCS) AND, AS SHOWN BELOW, HAVE LITTLE PREDICTIVE POWER. GCS VARIABLES ARE MUTUALLY EXCLUSIVE FROM GCS_TOTAL SO WE NEED TO PICK ON OR THE OTHER. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## DROPPING INDIVIDUAL COLUMNS FOR WHICH THERE IS LITTLE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# THE FOLLOWING BLOCK OF CODE PLOTS COLORMAPS OF MISSING DATA POINTS. AFTER HAVING GONE THROUGH THAT PROCESS\n",
    "# I'VE STEPPED BACK UP TO THIS POINT TO DROP COLUMNS THAT WERE SHOWN TO BE VERY SPARSE. THEN I'M PLOTTING THE \n",
    "# HEATMAPS AND CORRELATION MATRICES AGAIN TO SEE HOW MUCH DATA WE'RE LEFT WITH WHEN NAN'S ARE DROPPED. THIS IS AN \n",
    "# ITERATIVE PROCESS. \n",
    "# THE COLUMNS BEING DROPPED WERE IDENTIFIED AS SPARSE IN HEATMAPS BELOW\n",
    "drop_cols = [x for x in data3.columns if (('O2_Fraction' in x) | (('TempC' in x) & ('TempC_Calc' not in x))) ]\n",
    "more_cols = ['Creat2_skew', 'Hg_skew', 'RR_Spont_skew', 'RR_Total_skew']\n",
    "for col in more_cols:\n",
    "    drop_cols.append(col)\n",
    "data3.drop(drop_cols, inplace = True, axis = 1)\n",
    "data3.set_index(np.arange(data3.shape[0]), inplace = True)\n",
    "print \"Complete\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOCK CONTINUOUS DATA BASED ON AFFINITY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DISPLAY COLORMAP OF DATA WHERE VALUES ARE TRANSFORMED TO 1 IF NAN, 0 OTHERWISE. \n",
    "# THIS SHOWS THE PRESENCE OF MISSING VALUES AND ENABLES US TO GET SOME IDEA ABOUT\n",
    "# WHICH VARIABLES ARE MOST OFTEN PRESENT OR MISSING TOGETHER. ALSO DISPLAYING \n",
    "# CORRELATIN COEFFICIENTS BETWEEN MISSING VALUES \n",
    "\n",
    "calc_list = ['mean', 'med', 'std', 'skew', 'min', 'max', 'first', 'slope', 'delta']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for calc in calc_list:\n",
    "    plt.figure(figsize= (30,30))\n",
    "    cols = [x for x in data3.columns if calc in x] \n",
    "    cols.sort()\n",
    "    \n",
    "    header = data3[cols]\n",
    "    for col in header.columns:\n",
    "        max_val = -1000 #header[col].max()*100\n",
    "        #header[col].replace(np.nan, max_val, inplace = True)\n",
    "        header[col] = header[col].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "    display(data3[cols].dropna().shape[0])\n",
    "    missing = header.corr()\n",
    "    display(missing[missing >= 0.7])\n",
    "    plt.pcolor(header)\n",
    "    plt.xticks(np.arange(0.5, len(header.columns), 1), header.columns)\n",
    "    plt.show()\n",
    "\n",
    "print \"Complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DISPLAY COLORMAP OF DATA WHERE VALUES ARE TRANSFORMED TO 1 IF NAN, 0 OTHERWISE. \n",
    "# THIS SHOWS THE PRESENCE OF MISSING VALUES AND ENABLES US TO GET SOME IDEA ABOUT\n",
    "# WHICH VARIABLES ARE MOST OFTEN PRESENT OR MISSING TOGETHER. ALSO DISPLAYING \n",
    "# CORRELATIN COEFFICIENTS BETWEEN MISSING VALUES \n",
    "\n",
    "calc_list = ['mean', 'med', 'std', 'skew', 'min', 'max', 'first', 'slope', 'delta']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize= (30,30))\n",
    "cols = [x for x in data3.columns if (('mean' in x) | ('Height' in x) | ('Weight' in x))] \n",
    "cols.sort()\n",
    "    \n",
    "header = data3[cols]\n",
    "for col in header.columns:\n",
    "    max_val = -1000 #header[col].max()*100\n",
    "        #header[col].replace(np.nan, max_val, inplace = True)\n",
    "    header[col] = header[col].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "display(data3[cols].dropna().shape[0])\n",
    "missing = header.corr()\n",
    "display(missing[missing >= 0.7])\n",
    "plt.pcolor(header)\n",
    "plt.xticks(np.arange(0.5, len(header.columns), 1), header.columns)\n",
    "plt.show()\n",
    "\n",
    "print \"Complete\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BREAKING UP CONTINUOUS VARIABLES INTO AFFINITY GROUPS TO MAXIMIZE THE SAMPLES AVAILABLE FOR FEATURE SELECTION / EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BREAKING UP VARIABLES SO THAT WE CAN DROP NAN VALUES AND STILL HAVE SUFFICIENT SAMPLES \n",
    "# TO TRANSFORM AND DO FEATURE SELECTION / SCORING\n",
    "# WILL NEED TO MERGE LATER IN A WAY THAT PROVIDES ADEQUATE SAMPLES\n",
    "\n",
    "cols1 = [x for x in data3.columns if ('BP' in x)]\n",
    "cols2 = [x for x in data3.columns if (('Creat2' in x) | ('Gluc' in x) | ('Hg' in x) | ('Hemat' in x) | ('TempC' in x))]\n",
    "cols3 = [x for x in data3.columns if ((('RR' in x) & ('Spont' not in x) & ('Total' not in x)) | ('HR' in x))]\n",
    "cols4 = [x for x in data3.columns if ('pH' in x)]\n",
    "header = ['hospital_expire_flag', 'subject_id', 'icustay_id']\n",
    "for thing in header:\n",
    "    cols1.insert(0, thing)\n",
    "    cols2.insert(0, thing) \n",
    "    cols3.insert(0, thing)\n",
    "    cols4.insert(0, thing)\n",
    "\n",
    "#display(cols1)\n",
    "data3.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "BP_data = data3[cols1].dropna()\n",
    "print \"BP_data: Shape = \"\n",
    "display(BP_data.shape)\n",
    "CreatGlucHgHmT_data = data3[cols2].dropna()\n",
    "print \"CreatGlucHgHmT_data: Creat, Gluc, HG, Hemat, Temp data. Shape = \"\n",
    "display(CreatGlucHgHmT_data.shape)\n",
    "HR_RR_data = data3[cols3].dropna()\n",
    "print \"BP_data: RR HR data. Shape = \"\n",
    "display(HR_RR_data.shape)\n",
    "pH_data = data3[cols4].dropna()\n",
    "print \"pH_data: pH data.Shape = \"\n",
    "display(pH_data.shape)                                     \n",
    "cont_frames = [BP_data, CreatGlucHgHmT_data, HR_RR_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame = pH_data\n",
    "pd.scatter_matrix(frame[frame.columns[3:]], alpha = 0.3, figsize = (20,20), diagonal = 'kde')\n",
    "#for frame in cont_frames:\n",
    "    #pd.scatter_matrix(frame[frame.columns[3:]], alpha = 0.3, figsize = (20,20), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CALCULATING THE QUARTILES ON THE DISTRIBUTIONS AND BINNING DATA INTO 4 BUCKETS\n",
    "# TO CONVERT CONTINUOUS VARIABLES TO CATEGORICAL\n",
    "\n",
    "def quant_cats(feature, Q1, Q2, Q3):\n",
    "    if feature <=Q1:\n",
    "        return 'Q0'\n",
    "    elif (feature >Q1 and feature <= Q2):\n",
    "        return 'Q1'\n",
    "    elif (feature > Q2 and feature <= Q3):\n",
    "        return 'Q2'\n",
    "    elif feature > Q3:\n",
    "        return 'Q3'\n",
    "    \n",
    "\n",
    "BP_cat_data = BP_data.copy()\n",
    "CreatGlucHgHmT_cat_data = CreatGlucHgHmT_data.copy()\n",
    "HR_RR_cat_data = HR_RR_data.copy()\n",
    "pH_cat_data = pH_data.copy()\n",
    "cont_cat_frames = [BP_cat_data, CreatGlucHgHmT_cat_data, HR_RR_cat_data, pH_cat_data]\n",
    "\n",
    "for frame in cont_cat_frames:\n",
    "    frame_stats = frame.describe()\n",
    "    for col in frame.columns[3:]:\n",
    "        Q1 = frame_stats[col].loc['25%']\n",
    "        Q2 = frame_stats[col].loc['50%']\n",
    "        Q3 = frame_stats[col].loc['75%']\n",
    "        frame[col] = frame[col].apply(lambda x: quant_cats(x, Q1, Q2, Q3))\n",
    "\n",
    "BP_cat_data.head()        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CONVERT CONTINUOUS/CATEGORICAL DATA TO DUMMIES\n",
    "dummies = data3[data3.columns[:3]]\n",
    "dummies.set_index(['icustay_id'], inplace = True)\n",
    "#display(monkey.head())\n",
    "for col in cat_dict.keys():\n",
    "    col2 = cat_dict_names[col]\n",
    "    chimp = pd.get_dummies(cat_dict[col][col2], prefix = cat_dict[col][col2].name)\n",
    "    dummies = dummies.merge(chimp, left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cont_cat_frames = [BP_cat_data, CreatGlucHgHmT_cat_data, HR_RR_cat_data]\n",
    "#cont_dummy_frames = \n",
    "\n",
    "\n",
    "BP_dummies = BP_cat_data[BP_cat_data.columns[:3]].merge(pd.get_dummies(BP_cat_data[BP_cat_data.columns[3:]]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "CreatGlucHgHmT_dummies = CreatGlucHgHmT_cat_data[CreatGlucHgHmT_cat_data.columns[:3]].merge(pd.get_dummies(CreatGlucHgHmT_cat_data[CreatGlucHgHmT_cat_data.columns[3:]]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "HR_RR_dummies = HR_RR_cat_data[HR_RR_cat_data.columns[:3]].merge(pd.get_dummies(HR_RR_cat_data[HR_RR_cat_data.columns[3:]]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "pH_dummies = pH_cat_data[pH_cat_data.columns[:3]].merge(pd.get_dummies(pH_cat_data[pH_cat_data.columns[3:]]), left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "                       \n",
    "display(BP_dummies.head())\n",
    "display(CreatGlucHgHmT_dummies.head())\n",
    "display(HR_RR_dummies.head())\n",
    "display(pH_dummies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_dummy_frames = [Cap_dummies, GCS_Total_dummies, GCS_dummies]\n",
    "cont_dummy_frames = [BP_dummies, CreatGlucHgHmT_dummies, HR_RR_dummies, pH_dummies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COULD BE DONE EARLIER IN PROCESS \n",
    "for frame in cont_dummy_frames:\n",
    "    frame.set_index('icustay_id', inplace = True)\n",
    "    frame.drop('subject_id', inplace = True, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_frames = [BP_dummies, CreatGlucHgHmT_dummies, HR_RR_dummies,pH_dummies, GCS_Total_dummies, GCS_dummies]\n",
    "dummy_frame_filenames = ['Chart_BP_Features', 'Chart_CreatGlucHgHmT_Features', 'Chart_HrRr_Features', \n",
    "                     'Chart_GCSTotal_Features','Chart_GCS_Features']\n",
    "dummy_dict = dict(zip(dummy_frame_filenames, dummy_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## DOUBLE CHECKING CHI2 CALCULATIONS\n",
    "X = BP_dummies[BP_dummies.columns[1:]]\n",
    "y = BP_dummies[BP_dummies.columns[0]]\n",
    "chichi, pval  = chi2(X,y) \n",
    "pval.sort()\n",
    "display(pval[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CREATGLUC ETC HAS ONLY 874 SAMPLES AND SO WON'T BE HELPFUL. \n",
    "\n",
    "root = os.getcwd() + '/features/'\n",
    "\n",
    "for name, frame in dummy_dict.iteritems():#frame = cat_dummy_frames[0]\n",
    "    X_continuous = frame[frame.columns[1:]]\n",
    "    y = frame['hospital_expire_flag']\n",
    "    #display(X_continuous.shape)\n",
    "    #display(y.shape)\n",
    "    # ONLY PASSING FRAMES W/ > 5000 ICUSTAYS\n",
    "    if y.shape[0] > 5000:\n",
    "        \n",
    "        # SELECT K BEST FEATURES BASED ON CHI2 SCORES\n",
    "        selector = SelectKBest(score_func = chi2, k = 'all')\n",
    "        selector.fit(X_continuous, y)\n",
    "        p_vals = pd.Series(selector.pvalues_, name = 'p_values', index = X_continuous.columns)\n",
    "        scores = pd.Series(selector.scores_, name = 'scores', index = X_continuous.columns)\n",
    "        cont_features_df = pd.concat([p_vals, scores], axis = 1)\n",
    "        cont_features_df.sort_values(by ='scores', ascending = False, inplace = True)\n",
    "        best_features = frame[cont_features_df[cont_features_df.p_values < .001].index]\n",
    "        frame = pd.DataFrame(y).merge(best_features, left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)\n",
    "        print \"{}     {}\".format(name, frame.shape)\n",
    "        frame.to_csv(root + name + '.csv')\n",
    "        cont_features_df[cont_features_df.p_values < .001].to_csv(root + name + 'Scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XXXXXXXXXXXXXXXXXXXXXXXXX \n",
    "## LEGACY CODE STARTS HERE \n",
    "## XXXXXXXXXXXXXXXXXXXXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, frame in dummy_dict.iteritems():\n",
    "    print \"{}     {}\".format(name, frame.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for frame in cont_dummy_frames:#frame = cat_dummy_frames[0]\n",
    "    X_continuous = frame[frame.columns[1:]]\n",
    "    y = frame['hospital_expire_flag']\n",
    "    display(X_continuous.shape)\n",
    "    display(y.shape)\n",
    "\n",
    "    # SELECT K BEST FEATURES BASED ON CHI2 SCORES\n",
    "    selector = SelectKBest(score_func = chi2, k = 'all')\n",
    "    selector.fit(X_continuous, y)\n",
    "    p_vals = pd.Series(selector.pvalues_, name = 'p_values', index = X_continuous.columns)\n",
    "    scores = pd.Series(selector.scores_, name = 'scores', index = X_continuous.columns)\n",
    "    cont_features_df = pd.concat([p_vals, scores], axis = 1)\n",
    "    cont_features_df.sort_values(by ='scores', ascending = False, inplace = True)\n",
    "    display(cont_features_df[cont_features_df.p_values < 0.05])\n",
    "    \n",
    "    feats = len(X_continuous.columns[1:])\n",
    "    if feats > 6: \n",
    "        feats = 6\n",
    "    else: \n",
    "        feats = len(X_continuous.columns[1:])\n",
    "    largest = pd.Series(selector.scores_).nlargest(feats)\n",
    "    X_best = X_continuous[X_continuous.columns[largest.index]]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y).merge(X_best, left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# ITERATING THROUGH EACH BLOCK, CALCULATING BEST FEATURES USING CHI2 COMPARISON, SELECTING UP TO 6 BEST\n",
    "# AND RUNNING EACH THROUGH CLASSIFIERS\n",
    "\n",
    "\n",
    "for frame in dummy_frames:  \n",
    "    X_continuous = frame[frame.columns[1:]]\n",
    "    y = frame['hospital_expire_flag']\n",
    "    display(X_continuous.shape)\n",
    "    display(y.shape)\n",
    "\n",
    "    # SELECT K BEST FEATURES BASED ON CHI2 SCORES\n",
    "    selector = SelectKBest(score_func = f_classif, k = 'all')\n",
    "    selector.fit(X_continuous, y)\n",
    "    \n",
    "    feats = len(X_continuous.columns[1:])\n",
    "    if feats > 10: \n",
    "        feats = 10\n",
    "    \n",
    "    largest = pd.Series(selector.scores_).nlargest(feats)\n",
    "    X_best = X_continuous[X_continuous.columns[largest.index]]\n",
    "    y_best = y\n",
    "\n",
    "    '''\n",
    "    p_vals = pd.Series(selector.pvalues_, name = 'p_values', index = X_continuous.columns)\n",
    "    scores = pd.Series(selector.scores_, name = 'scores', index = X_continuous.columns)\n",
    "    cont_features_df = pd.concat([p_vals, scores], axis = 1)\n",
    "    if loop < 1:\n",
    "        cont_feature_scores = cont_features_df\n",
    "    else: \n",
    "        cont_feature_scores = cont_feature_scores.append(cont_features_df)\n",
    "    loop += 1  \n",
    "    '''\n",
    "    #cont_feature_scores.sort_values(by ='scores', ascending = False, inplace = True)\n",
    "    #display(cont_feature_scores.head(40))\n",
    "\n",
    "    print \"************************************************************************\"\n",
    "    display(X_best.columns[0:10])\n",
    "    print \"Dataset Shape = {}\".format(X_best.shape)\n",
    "    print \"Survival rate for this dataset is {}\".format(1-float(y.sum())/y.shape[0])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_best, y, test_size = 0.10, random_state = 42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    deathweight = 1/(float(y.sum())/y.shape[0])\n",
    "    print \"The proportion of live to dead patients = {}\".format(deathweight)\n",
    "    \n",
    "    \n",
    "    \n",
    "    clf_KNC = KNeighborsClassifier()\n",
    "    clf_KNC.fit(X_train, y_train)\n",
    "    scores = cross_val_score(clf_KNC, X_best, y, cv=5)\n",
    "    print \"Scores from KNeighbors Classificaiton\"\n",
    "    display(scores)  \n",
    "    \n",
    "    clf_KNC.score(X_test, y_test)\n",
    "    y_predsKNC = clf_KNC.predict(X_test)\n",
    "    print \"Confusion matrix for KNeighbors Classification\"\n",
    "    display(metrics.confusion_matrix(y_test, y_predsKNC))\n",
    "    print \"/n/n\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    clf_SVC = svm.SVC(C=1.0, class_weight='balanced', max_iter=-1, \n",
    "        tol=0.001, verbose=False, random_state = 42).fit(X_train, y_train)\n",
    "    clf_SVC.fit(X_train, y_train)\n",
    "    scores = cross_val_score(clf_SVC, X_best, y, cv=5)\n",
    "    print \"Scores from Linear Support Vector Classificaiton\"\n",
    "    display(scores)  \n",
    "    \n",
    "    clf_SVC.score(X_test, y_test)\n",
    "    y_predsSVC = clf_SVC.predict(X_test)\n",
    "    print \"Confusion matrix for Linear Support Vector Classification\"\n",
    "    display(metrics.confusion_matrix(y_test, y_predsSVC))\n",
    "    print \"/n/n\"\n",
    "    \n",
    "    \n",
    "    clf_Tree = DecisionTreeClassifier(random_state = 0, max_features = feats)\n",
    "    scores = cross_val_score(clf_Tree, X_best, \n",
    "                         y, cv=5)\n",
    "    print \"Scores from Decision Tree Classifier\"\n",
    "    display(scores)  \n",
    "\n",
    "    clf_Tree.fit(X_train, y_train)\n",
    "    clf_Tree.score(X_test, y_test)\n",
    "    y_predsTree = clf_Tree.predict(X_test)\n",
    "    print \"Confusion matrix for Decision Tree\"\n",
    "    display(metrics.confusion_matrix(y_test, y_predsTree))\n",
    "\n",
    "\n",
    "\n",
    "    ### create classifier\n",
    "    clf_GNB = GaussianNB()\n",
    "    ### fit the classifier on the training features and labels\n",
    "    clf_GNB.fit(X_train, y_train)\n",
    "    ### return the fit classifier\n",
    "    print \"Gaussian Naive Bayes Classifier Score\"\n",
    "    nb_score = clf_GNB.score(X_test, y_test) \n",
    "\n",
    "    print nb_score\n",
    "    y_predsGNB = clf_GNB.predict(X_test)\n",
    "    print \"Confusion Matrix for Gaussian Naive Bayes Classifier\"\n",
    "    metrics.confusion_matrix(y_test, y_predsGNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#cat_dummy_frames = [Cap_dummies, GCS_Total_dummies, GCS_dummies]\n",
    "\n",
    "\n",
    "for frame in cont_dummy_frames:  \n",
    "    X_continuous = frame[frame.columns[1:]]\n",
    "    y = frame['hospital_expire_flag']\n",
    "    display(X_continuous.shape)\n",
    "    display(y.shape)\n",
    "\n",
    "    # SELECT K BEST FEATURES BASED ON CHI2 SCORES\n",
    "    selector = SelectKBest(score_func = chi2, k = 'all')\n",
    "    selector.fit(X_continuous, y)\n",
    "    \n",
    "    feats = len(X_continuous.columns[1:])\n",
    "    if feats > 6: \n",
    "        feats == 6\n",
    "    largest = pd.Series(selector.scores_).nlargest(feats)\n",
    "    X_best = X_continuous[X_continuous.columns[largest.index]]\n",
    "    y_best = y\n",
    "\n",
    "    '''\n",
    "    p_vals = pd.Series(selector.pvalues_, name = 'p_values', index = X_continuous.columns)\n",
    "    scores = pd.Series(selector.scores_, name = 'scores', index = X_continuous.columns)\n",
    "    cont_features_df = pd.concat([p_vals, scores], axis = 1)\n",
    "    if loop < 1:\n",
    "        cont_feature_scores = cont_features_df\n",
    "    else: \n",
    "        cont_feature_scores = cont_feature_scores.append(cont_features_df)\n",
    "    loop += 1  \n",
    "    '''\n",
    "    #cont_feature_scores.sort_values(by ='scores', ascending = False, inplace = True)\n",
    "    #display(cont_feature_scores.head(40))\n",
    "\n",
    "    print \"************************************************************************\"\n",
    "    display(X_best.columns[0:10])\n",
    "   \n",
    "    print \"Survival rate for this dataset is {}\".format(1-float(y.sum())/y.shape[0])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_best, y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    clf_SVC = svm.SVC(C=1.0, cache_size=200, class_weight={1 : 3, 0 : 1}, coef0=0.0,\n",
    "        decision_function_shape='ovo', degree=3, gamma='auto', kernel='poly', \n",
    "        max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "        tol=0.001, verbose=False).fit(X_train, y_train)\n",
    "\n",
    "    scores = cross_val_score(clf_SVC, X_best, y, cv=5)\n",
    "    print \"Scores from Support Vector Classificaiton\"\n",
    "    display(scores)  \n",
    "\n",
    "\n",
    "    clf_SVC.score(X_test, y_test)\n",
    "    y_predsSVC = clf_SVC.predict(X_test)\n",
    "    print \"Confusion matrix for Support Vector Classification\"\n",
    "    display(metrics.confusion_matrix(y_test, y_predsSVC))\n",
    "\n",
    "    clf_Tree = DecisionTreeClassifier(random_state = 0, max_features = feats)\n",
    "    scores = cross_val_score(clf_Tree, X_best, \n",
    "                         y, cv=5)\n",
    "    print \"Scores from Decision Tree Classifier\"\n",
    "    display(scores)  \n",
    "\n",
    "    clf_Tree.fit(X_train, y_train)\n",
    "    clf_Tree.score(X_test, y_test)\n",
    "    y_predsTree = clf_Tree.predict(X_test)\n",
    "    print \"Confusion matrix for Decision Tree\"\n",
    "    display(metrics.confusion_matrix(y_test, y_predsTree))\n",
    "\n",
    "\n",
    "\n",
    "    ### create classifier\n",
    "    clf_GNB = GaussianNB()\n",
    "    ### fit the classifier on the training features and labels\n",
    "    clf_GNB.fit(X_train, y_train)\n",
    "    ### return the fit classifier\n",
    "    print \"Gaussian Naive Bayes Classifier Score\"\n",
    "    nb_score = clf_GNB.score(X_test, y_test) \n",
    "\n",
    "    print nb_score\n",
    "    y_predsGNB = clf_GNB.predict(X_test)\n",
    "    print \"Confusion Matrix for Gaussian Naive Bayes Classifier\"\n",
    "    metrics.confusion_matrix(y_test, y_predsGNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(list(cont_feature_scores.head(10).index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loop = 0\n",
    "\n",
    "for frame in cont_dummy_frames:\n",
    "    new_frame = frame.copy()\n",
    "    new_frame.set_index('icustay_id', inplace = True)\n",
    "    cols_list = [x for x in list(cont_feature_scores.head(10).index) if x in list(new_frame.columns)]\n",
    "    if loop < 1:\n",
    "        cols_list.insert(0,'hospital_expire_flag')\n",
    "        #display(cols_list)\n",
    "        #display(new_frame.columns)\n",
    "        best_feats = new_frame[cols_list]\n",
    "    else:\n",
    "        best_feats = best_feats.merge(new_frame[cols_list], left_index = True, right_index = True, \n",
    "                       how = 'inner', sort = True)\n",
    "\n",
    "    loop += 1\n",
    "display(best_feats.shape)\n",
    "display(best_feats.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "float(HR_RR_dummies['hospital_expire_flag'].sum())/HR_RR_dummies.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BP_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(BP_dummies[BP_dummies.columns[3:]], \n",
    "                                                    BP_dummies['hospital_expire_flag'], \n",
    "                                                    test_size = 0.30, random_state = 42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf_SVC = svm.SVC(C=1.0, cache_size=200, class_weight={1 : 3, 0 : 1}, coef0=0.0,\n",
    "    decision_function_shape='ovo', degree=2, gamma='auto', kernel='poly', \n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False).fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(clf_SVC, HR_RR_dummies[cols], \n",
    "                         HR_RR_dummies['hospital_expire_flag'], cv=5)\n",
    "display(scores)  \n",
    "\n",
    "\n",
    "clf_SVC.score(X_test, y_test)\n",
    "y_predsSVC = clf_SVC.predict(X_test)\n",
    "display(metrics.confusion_matrix(y_test, y_predsSVC))\n",
    "\n",
    "clf_Tree = DecisionTreeClassifier(random_state = 0, max_features = 6)\n",
    "scores = cross_val_score(clf_Tree, boxcox_dummies2[boxcox_dummies2.columns[2:]], \n",
    "                         boxcox_dummies2['hospital_expire_flag'], cv=5)\n",
    "display(scores)  \n",
    "\n",
    "clf_Tree.fit(X_train, y_train)\n",
    "clf_Tree.score(X_test, y_test)\n",
    "y_predsTree = clf_Tree.predict(X_test)\n",
    "display(metrics.confusion_matrix(y_test, y_predsTree))\n",
    "\n",
    "\n",
    "\n",
    "### create classifier\n",
    "clf_GNB = GaussianNB()\n",
    "### fit the classifier on the training features and labels\n",
    "clf_GNB.fit(X_train, y_train)\n",
    "    ### return the fit classifier\n",
    "\n",
    "nb_score = clf_GNB.score(X_test, y_test) \n",
    "\n",
    "print nb_score\n",
    "y_predsGNB = clf_GNB.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_predsGNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cont_feature_scores.sort_values(by ='p_values', ascending = True)\n",
    "cont_feature_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data3.set_index(['icustay_id'], inplace = True)\n",
    "data4.set_index(['icustay_id'], inplace = True)\n",
    "data4.drop('subject_id')\n",
    "\n",
    "# ADD CODE TO LABEL LAB VS CHART CHART TIME\n",
    "data3 = data3.merge(data4, left_index = True, right_index = True, \n",
    "                       how = 'left', sort = True, suffixes = ('_chart', '_lab'))\n",
    "\n",
    "data3['icustay_id'] = data3.index\n",
    "cols = list(data3.columns)\n",
    "cols.sort()\n",
    "cols.insert(0, cols.pop(cols.index('icustay_id')))\n",
    "cols.insert(1, cols.pop(cols.index('subject_id')))\n",
    "cols.insert(2, cols.pop(cols.index('Height')))\n",
    "cols.insert(3, cols.pop(cols.index('Weight')))\n",
    "cols.insert(4, cols.pop(cols.index('charttime_x')))\n",
    "cols.insert(5, cols.pop(cols.index('charttime_y')))\n",
    "cols.insert(6, cols.pop(cols.index('intime_x')))\n",
    "cols.insert(7, cols.pop(cols.index('intime_y')))\n",
    "cols.insert(8, cols.pop(cols.index('outtime_x')))\n",
    "cols.insert(9, cols.pop(cols.index('outtime_y')))\n",
    "\n",
    "data3 = data3[cols]\n",
    "data3.set_index(np.arange(data3.shape[0]), inplace = True)\n",
    "\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data3.rename(columns = {'charttime_x':'charttime_chart', 'charttime_y':'charttime_lab', \n",
    "              'intime_x':'intime_chart', 'intime_y':'intime_lab', \n",
    "             'outtime_x':'outtime_chart', 'outtime_y':'outtime_lab'}, inplace = True)\n",
    "data3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data4 = data3.dropna(thresh = 57)\n",
    "data5 = data3.dropna(subset = ['BP_Dia_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in data5.columns:\n",
    "    print \"{} has  {} NaN values\".format(col,data5[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data5.dropna(thresh = 50).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in data4.columns:\n",
    "    print \"{} has  {} NaN values\".format(col,data4[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data3.drop(['admittime', 'intime', 'charttime', 'itemid', 'cgid', 'value', \n",
    "#            'valuenum', 'valueuom', 'label'], axis = 1, inplace = True)\n",
    "#data3.drop(['itemid', 'angus'], axis = 1, inplace = True) \n",
    "#            'valuenum', 'valueuom', 'label'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(data[(data.label=='WBC') & (data.valuenum.isnull())].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"CO        {}\".format(CO_df.shape)\n",
    "print \"WBC       {}\".format(WBC_df.shape)\n",
    "print \"RR        {}\".format(RR_df.shape)\n",
    "print \"MAP       {}\".format(MAP_df.shape)\n",
    "print \"HR        {}\".format(HR_df.shape)\n",
    "print \"HR2       {}\".format(HR2_df.shape)\n",
    "print \"CVP_med   {}\".format(CVP_med_df.shape)\n",
    "print \"CVP_min   {}\".format(CVP_min_df.shape)\n",
    "print \"pH        {}\".format(pH_df.shape)\n",
    "print \"Lac_mean  {}\".format(LAC_mean_df.shape)\n",
    "print \"Lac_min   {}\".format(LAC_min_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ptnt_demog = pd.DataFrame.from_csv('PTNT_DEMOG_ANGUS_rev.csv')\n",
    "ptnt_demog.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(data3.columns)\n",
    "display(ptnt_demog.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ptnt_demog2 = ptnt_demog[['gender', 'marital_status', 'ethnicity', 'insurance', 'first_careunit', \n",
    "                          'age', 'hospital_expire_flag']]\n",
    "ptnt_demog2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the patient demographic and summary chart data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data4 = data3.copy()\n",
    "data4 = data4.merge(ptnt_demog2, left_index = True, right_index = True, how='left', sort = True)\n",
    "data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data4.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df[np.isfinite(df['EPS'])]\n",
    "#data4[np.isfinite(data4['CO_max'])].shape\n",
    "#data5 = data4.drop(['CO_max', 'HR2_mean'], axis = 1)\n",
    "# DROP COLUMNS WHERE THERE ARE (ARBITRARILY) < 7600 VALUES\n",
    "data5 = data4[data4.columns[data4.isnull().sum() < 7600]]\n",
    "data5.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data5.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "float_vars = data5.dtypes[data5.dtypes == 'float'].index\n",
    "cat_vars = data5.dtypes[data5.dtypes == 'object'].index\n",
    "int_vars = data5.dtypes[data5.dtypes == 'int64'].index\n",
    "cat_vars = cat_vars.drop(['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#int_vars = int_vars.drop(['subject_id', 'hospital_expire_flag', 'angus'])\n",
    "#int_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HR2\n",
    "display(data5.dropna().shape)\n",
    "display(data5[data5.hospital_expire_flag == 1].dropna().shape)\n",
    "display(data5[data5.hospital_expire_flag == 0].dropna().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for thing in cat_vars[1:2]:\n",
    "    cats = list(data5[thing].unique())\n",
    "    print cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for thing in cat_vars[:2]:\n",
    "    datadict = dict()\n",
    "    cats = list(data5[thing].unique())\n",
    "    print thing\n",
    "    print cats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for thing in cat_vars:\n",
    "    datadict = dict()\n",
    "    cats = list(data5[thing].unique())\n",
    "    print thing\n",
    "    print cats\n",
    "\n",
    "    for cat in cats:\n",
    "        dead = data5[thing][(data5[thing] == cat) & (data5.hospital_expire_flag == 1)].dropna().count()\n",
    "        live = data5[thing][(data5[thing] == cat) & (data5.hospital_expire_flag == 0)].dropna().count()\n",
    "        total = float(live) + dead\n",
    "        datadict[cat] = (live/total, dead/total)\n",
    "\n",
    "    frame = pd.DataFrame.from_dict(datadict)\n",
    "    frame.index = ['Survivors', 'Non_Survivors']\n",
    "    \n",
    "    frame.transpose().plot.bar(stacked = True, figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                 alpha = 0.5, title = \"Percent Survival Rate for \" + thing)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(int_vars)\n",
    "int_vars[1:len(int_vars)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for thing in int_vars[1:len(int_vars)-1]:\n",
    "    datadict = dict()\n",
    "    cats = list(data5[thing].unique())\n",
    "    print thing\n",
    "    print cats\n",
    "\n",
    "    for cat in cats:\n",
    "        dead = data5[thing][(data5[thing] == cat) & (data5.hospital_expire_flag == 1)].dropna().count()\n",
    "        live = data5[thing][(data5[thing] == cat) & (data5.hospital_expire_flag == 0)].dropna().count()\n",
    "        #total = float(live) + dead\n",
    "        #datadict[cat] = (live/total, dead/total)\n",
    "        datadict[cat] = (live, dead)\n",
    "    frame = pd.DataFrame.from_dict(datadict)\n",
    "    frame.index = ['Survivors', 'Non_Survivors']\n",
    "    \n",
    "    frame.transpose().plot.bar(stacked = True, figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                 alpha = 0.5, title = \"Percent Survival Rate for \" + thing)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "float_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for thing in float_vars:\n",
    "    # plot\n",
    "    plt.subplots(figsize=(13,6))\n",
    "    data5[thing][data5.hospital_expire_flag==1].dropna().plot.kde(\n",
    "        alpha=1.0,label='Non-survival')\n",
    "    data5[thing][data5.hospital_expire_flag==0].dropna().plot.kde(\n",
    "        alpha=1.0,label='Survival')\n",
    "    \n",
    "  \n",
    "\n",
    "    # add title, labels etc.\n",
    "    plt.title('First {} measurement on ICU admission '.format(thing.lower()) +\n",
    "               'vs ICU mortality \\n')\n",
    "    plt.xlabel(thing)\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1),fontsize=12)\n",
    "   # plt.xlim(0, data5[thing].dropna().quantile(0.99))\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dead_men = data5.gender[(data5['gender']=='M') & (data5.hospital_expire_flag ==1)].dropna().count()\n",
    "dead_women = data5.gender[(data5['gender']=='F') & (data5.hospital_expire_flag ==1)].dropna().count()\n",
    "live_men = data5.gender[(data5['gender']=='M') & (data5.hospital_expire_flag ==0)].dropna().count()\n",
    "live_women = data5.gender[(data5['gender']=='F') & (data5.hospital_expire_flag ==0)].dropna().count()\n",
    "survival = pd.DataFrame([[live_men, live_women], \n",
    "                        [dead_men, dead_women]], \n",
    "                       columns = ['Men', 'Women'],\n",
    "                       index = ['Survivors', 'Non_Survivors'])\n",
    "survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "survival.transpose().plot.bar(stacked = True, figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                             alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.arange(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind = np.arange(1,3)    # the x locations for the groups\n",
    "width = 0.25       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "plt.figure(figsize= (20,10))\n",
    "p1 = plt.bar(ind - width/2, survival.iloc[0], width, color = 'green', edgecolor = 'black',\n",
    "            linewidth = 3)\n",
    "p2 = plt.bar(ind - width/2, survival.iloc[1], width, color = 'red', edgecolor = 'black', \n",
    "             linewidth = 3, bottom=survival.iloc[0])\n",
    "\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.title('Survivors vs. Non-Survivors')\n",
    "plt.xticks(ind, ('Men', 'Women'))\n",
    "#plt.yticks(np.arange(0, 81, 10))\n",
    "plt.legend((p1[0], p2[0]), ('Survivors', 'Non_Survivors'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ax = skew_df.plot.bar(figsize = (20,7), title = 'Skewness Values for Different Data Transforms')\n",
    "#ax.set_ylabel(\"Skewness\")\n",
    "#data5.groupby(['hospital_expire_flag'])[['gender']].count().plot(kind='bar', stacked = True )\n",
    "data5[cat_vars].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# a dictionary is created containing units of measure for the different lab measurements\n",
    "labs = {'ANIONGAP': 'mEq/L',\n",
    "        'ALBUMIN': 'g/dL',\n",
    "        'BICARBONATE': 'mEq/L',\n",
    "        'BILIRUBIN': 'mg/dL',\n",
    "        'BUN': '',\n",
    "        'CHLORIDE': 'mEq/L',\n",
    "        'CREATININE': 'mg/dL',\n",
    "        'GLUCOSE': 'mg/dL',\n",
    "        'HEMATOCRIT': '%',\n",
    "        'HEMOGLOBIN': 'g/dL',\n",
    "        'INR': '',\n",
    "        'LACTATE': 'mmol/L',\n",
    "        'MAGNESIUM': 'mmol/L',\n",
    "        'PHOSPHATE': 'mg/dL',\n",
    "        'PLATELET': 'K/uL',\n",
    "        'POTASSIUM': 'mEq/L',\n",
    "        'PT': '',\n",
    "        'PTT': 'sec',\n",
    "        'SODIUM':'mmol/L',\n",
    "        'WBC': ''}\n",
    "'''\n",
    "# a list is created containing units of measure for the lab measurements\n",
    "lab_units = ['g/dL',\n",
    "        'mEq/L',\n",
    "        'mEq/L',\n",
    "        'mg/dL',\n",
    "        '',\n",
    "        'mEq/L',\n",
    "        'mg/dL',\n",
    "        'mg/dL',\n",
    "        '%', \n",
    "        'g/dL',\n",
    "        '',\n",
    "        'mmol/L',\n",
    "        'mmol/L',\n",
    "        'mg/dL',\n",
    "        'K/uL',\n",
    "        'mEq/L',\n",
    "        '',\n",
    "        'sec',\n",
    "        'mmol/L',\n",
    "        '']\n",
    "\n",
    "# currently using lab_measures and lab_units to create the dict. \n",
    "# could be done more succinctly using the dictionary described above\n",
    "labs_dict = dict(zip(lab_measures, lab_units))\n",
    "print labs_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for l, u in iter(sorted(labs_dict.iteritems())):\n",
    "    # count patients\n",
    "    n_nonsurv = data[l][data.mort_icu==1].dropna().count()\n",
    "    n_surv = data[l][data.mort_icu==0].dropna().count()\n",
    "    \n",
    "    # get median, variance, skewness\n",
    "    med_nonsurv = data[l][data.mort_icu==1].dropna().median()\n",
    "    med_surv = data[l][data.mort_icu==0].dropna().median()\n",
    "    var_nonsurv = data[l][data.mort_icu==1].dropna().var()\n",
    "    var_surv = data[l][data.mort_icu==0].dropna().var()\n",
    "    skew_nonsurv = data[l][data.mort_icu==1].dropna().skew()\n",
    "    skew_surv = data[l][data.mort_icu==0].dropna().skew() \n",
    "    \n",
    "    # Are the 2 samples drawn from the same continuous distribution? \n",
    "    # Try Kolmogorov Smirnov test \n",
    "    ks_stat, p_val = ks_2samp(data[l][data.mort_icu==1].dropna(),\n",
    "                              data[l][data.mort_icu==0].dropna())\n",
    "\n",
    "    # plot\n",
    "    plt.subplots(figsize=(13,6))\n",
    "    data[l][data.mort_icu==1].dropna().plot.kde(\n",
    "        alpha=1.0,label='Non-survival (n={})'.format(n_nonsurv))\n",
    "    data[l][data.mort_icu==0].dropna().plot.kde(\n",
    "        alpha=1.0,label='Survival (n={})'.format(n_surv))\n",
    "    \n",
    "    # fake plots for KS test, median, etc\n",
    "    plt.plot([], label=' ',color='lightgray')\n",
    "    plt.plot([], label='KS test: p={}'.format(format(p_val,'.3f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Median (non-surv): {}'.format(format(med_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Median (surv): {}'.format(format(med_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Variance (non-surv): {}'.format(format(var_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Variance (surv): {}'.format(format(var_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Skew (non-surv): {}'.format(format(skew_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Skew (surv): {}'.format(format(skew_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "\n",
    "    # add title, labels etc.\n",
    "    plt.title('First {} measurement on ICU admission '.format(l.lower()) +\n",
    "               'vs ICU mortality \\n')\n",
    "    plt.xlabel(l + ' ' + u)\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1),fontsize=12)\n",
    "    plt.xlim(0, data[l].dropna().quantile(0.99))\n",
    "    \n",
    "    # Add lab range if available\n",
    "    if l in lab_ranges:\n",
    "        plt.axvline(lab_ranges[l][0],color='k',linestyle='--')\n",
    "        plt.axvline(lab_ranges[l][1],color='k',linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating box-plots for each variable for both survival and non-survival groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot first laboratory measurement on ICU admission vs ICU mortality\n",
    "# Would be clearer to plot CDF\n",
    "# Additional variables to be added: magnesium, phosphate, calcium\n",
    "\n",
    "labs = {'ANIONGAP': 'mEq/L',\n",
    "        'ALBUMIN': 'g/dL',\n",
    "        'BICARBONATE': 'mEq/L',\n",
    "        'BILIRUBIN': 'mg/dL',\n",
    "        'BUN': '',\n",
    "        'CHLORIDE': 'mEq/L',\n",
    "        'CREATININE': 'mg/dL',\n",
    "        'GLUCOSE': 'mg/dL',\n",
    "        'HEMATOCRIT': '%',\n",
    "        'HEMOGLOBIN': 'g/dL',\n",
    "        'INR': '',\n",
    "        'LACTATE': 'mmol/L',\n",
    "        'MAGNESIUM': 'mmol/L',\n",
    "        'PHOSPHATE': 'mg/dL',\n",
    "        'PLATELET': 'K/uL',\n",
    "        'POTASSIUM': 'mEq/L',\n",
    "        'PT': '',\n",
    "        'PTT': 'sec',\n",
    "        'SODIUM':'mmol/L',\n",
    "        'WBC': ''}\n",
    "\n",
    "for l, u in iter(sorted(labs.iteritems())):\n",
    "    # count patients\n",
    "    n_nonsurv = data[l.lower()+'_1st'][data.mort_icu==1].dropna().count()\n",
    "    n_surv = data[l.lower()+'_1st'][data.mort_icu==0].dropna().count()\n",
    "    \n",
    "    # get median, variance, skewness\n",
    "    med_nonsurv = data[l.lower()+'_1st'][data.mort_icu==1].dropna().median()\n",
    "    med_surv = data[l.lower()+'_1st'][data.mort_icu==0].dropna().median()\n",
    "    var_nonsurv = data[l.lower()+'_1st'][data.mort_icu==1].dropna().var()\n",
    "    var_surv = data[l.lower()+'_1st'][data.mort_icu==0].dropna().var()\n",
    "    skew_nonsurv = data[l.lower()+'_1st'][data.mort_icu==1].dropna().skew()\n",
    "    skew_surv = data[l.lower()+'_1st'][data.mort_icu==0].dropna().skew() \n",
    "    \n",
    "    # Are the 2 samples drawn from the same continuous distribution? \n",
    "    # Try Kolmogorov Smirnov test \n",
    "    ks_stat, p_val = ks_2samp(data[l.lower()+'_1st'][data.mort_icu==1].dropna(),\n",
    "                              data[l.lower()+'_1st'][data.mort_icu==0].dropna())\n",
    "\n",
    "    # plot\n",
    "    #fig, ax1 = plt.subplots(figsize=(13, 6))\n",
    "    #fig.canvas.set_window_title('A Boxplot Example')\n",
    "    #plt.subplots(figsize=(13,6))\n",
    "    #box_data = data[['mort_icu', l.lower()+'_1st']].dropna()\n",
    "    data[['mort_icu', l.lower()+'_1st']].dropna().boxplot(by='mort_icu', figsize = (13,5))\n",
    "    #data[['mort_icu', l.lower()+'_1st']].dropna().boxplot(by='mort_icu', figsize = (13,5))\n",
    "    #    label='Non-survival (n={})'.format(n_nonsurv))\n",
    "    plt.suptitle(\"\")\n",
    "    '''\n",
    "    data[l.lower()+'_1st'][data.mort_icu==0].dropna().plot.box(\n",
    "        label='Survival (n={})'.format(n_surv))\n",
    "    '''\n",
    "    \n",
    "    # fake plots for KS test, median, etc\n",
    "    plt.plot([], label=' ',color='lightgray')\n",
    "    plt.plot([], label='KS test: p={}'.format(format(p_val,'.3f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Median (non-surv): {}'.format(format(med_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Median (surv): {}'.format(format(med_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Variance (non-surv): {}'.format(format(var_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Variance (surv): {}'.format(format(var_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Skew (non-surv): {}'.format(format(skew_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Skew (surv): {}'.format(format(skew_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "    \n",
    "   # add title, labels etc.\n",
    "    plt.title('First {} measurement on ICU admission '.format(l.lower()) +\n",
    "               'vs ICU mortality \\n')\n",
    "    \n",
    "    plt.xlabel(l + ' ' + u)\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1),fontsize=12)\n",
    "   # plt.xlim(0, data[l.lower()+'_1st'].dropna().quantile(0.99))\n",
    "    \n",
    "    # Add lab range if available\n",
    "    if l in lab_ranges:\n",
    "        plt.axvline(lab_ranges[l][0],color='k',linestyle='--')\n",
    "        plt.axvline(lab_ranges[l][1],color='k',linestyle='--')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the density plots above, a list of variables was generated for which the distribution was very similar between survival and non-survival groups. (Maybe include similarity threshold??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate list of variables which, from density plots above, are similar between surivial and non-survival groups\n",
    "cols_list = list(data.columns)\n",
    "remove_list = ['chloride_1st', 'glucose_1st', 'hematocrit_1st', 'hemoglobin_1st', 'platelet_1st']\n",
    "for element in remove_list:\n",
    "    cols_list.remove(element)\n",
    "    \n",
    "display(cols_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data4 includes all variables except those identified as similar between survival and non-survival groups. \n",
    "data2 = data[cols_list]\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing data points that include any NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove samples which have any values that are nan\n",
    "data2 = data2.dropna()\n",
    "data2.set_index(np.arange(data2.shape[0]), inplace = True)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The survival rate calculated below indicates that 82% of patients survived. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dead = data2['mort_icu'][data2.mort_icu==1].count()\n",
    "survivors = data2['mort_icu'][data2.mort_icu==0].count()\n",
    "survival_rate = float(survivors)/(dead+survivors)\n",
    "print \"Number of  patients deceased = {}\".format(dead)\n",
    "print \"Number of patients           = {}\".format(dead + survivors)\n",
    "print \"Survival Rate                = {}\".format(survival_rate * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Classification on un-processed data with variables selected based on visual inspection\n",
    "The variables enumerated in 'remove_list' were observed for both survivors and non-survivors and appeared to be very similar in distribution. Because they appeared to be very similar for both groups, they were excluded as features for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data2[data2.columns[1:]], data2['mort_icu'], \n",
    "                                                    test_size = 0.30, random_state = 42)\n",
    "\n",
    "clf_SVC = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf', \n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False).fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(clf_SVC, data2[data2.columns[1:]], data2['mort_icu'], cv=5)\n",
    "display(scores)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_SVC.score(X_test, y_test)\n",
    "y_predsSVC = clf_SVC.predict(X_test)\n",
    "display(metrics.confusion_matrix(y_test, y_predsSVC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Confusion Matrix indicates that the classifier simply predicts survival for each patient which, because 82% of the patients survived, results in 82% Accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes Classification on un-processed data with variables selected based on visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data2[data2.columns[1:]], data2['mort_icu'], \n",
    "                                                    test_size = 0.40, random_state = 42)\n",
    "\n",
    "### create classifier\n",
    "clf_GNB = GaussianNB()\n",
    "### fit the classifier on the training features and labels\n",
    "clf_GNB.fit(X_train, y_train)\n",
    "    ### return the fit classifier\n",
    "\n",
    "nb_score = clf_GNB.score(X_test, y_test) \n",
    "\n",
    "print nb_score\n",
    "y_predsGNB = clf_GNB.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_predsGNB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Confusion Matrix indicates that the classifier predicts survival rates with ~78% accuracy with a mixture of predictions rather than predicting all survival as was the case for the SVM classifier. Unfortunately, the accuracy is not better than predicting all survivors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron Classification on un-processed data with variables selected based on visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data2[data2.columns[1:]], data2['mort_icu'], \n",
    "                                                    test_size = 0.20, random_state = 42)\n",
    "\n",
    "clf_MLP = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(7, 4), random_state=1)\n",
    "\n",
    "clf_MLP.fit(X_train, y_train)                         \n",
    "\n",
    "display(clf_MLP.score(X_test, y_test))\n",
    "y_predsMLP = clf_MLP.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_predsMLP)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataframe of skewness measurements for the raw data and for different transforms to calculate what transform does the best job of normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dict from which we'll build skew measures dataframe\n",
    "skew_dict2 = {data2.columns[1]:\n",
    "              {\n",
    "             'raw_skew': scats.skew(data2[data2.columns[1]]), \n",
    "             'sqrt_skew': scats.skew(np.sqrt(data2[data2.columns[1]])), \n",
    "             'log_skew': scats.skew(np.log(data2[data2.columns[1]].add(1))),  \n",
    "             'boxcox_skew': scats.skew(scats.boxcox(data2[data2.columns[1]].add(1))[0])\n",
    "             }\n",
    "             }\n",
    "\n",
    "print skew_dict2\n",
    "skew_df = pd.DataFrame.from_dict(skew_dict2, orient = 'index')  \n",
    "skew_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# re-order columns\n",
    "skew_df = skew_df[['raw_skew', 'sqrt_skew', 'log_skew', 'boxcox_skew']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skew_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "              \n",
    "for col in data2.columns[2:]:\n",
    "    raw_skew = scats.skew(data2[col])\n",
    "    \n",
    "    sqrt_skew = scats.skew(np.sqrt(data2[col]))\n",
    "    log_skew = scats.skew(np.log(data2[col].add(1)))\n",
    "        #print \"{} logskew = {}\".format(col, new_skew_val2)\n",
    "    boxcox_skew = scats.skew(scats.boxcox(data2[col].add(1))[0])\n",
    "    new_row = pd.Series([raw_skew, sqrt_skew, log_skew, boxcox_skew],\n",
    "                        #index=['measurement', 'raw_skew', 'sqrt_skew', 'log_skew', 'boxcox_skew'],\n",
    "                        index=['raw_skew', 'sqrt_skew', 'log_skew', 'boxcox_skew'],\n",
    "                       name = col)\n",
    "\n",
    "    skew_df = skew_df.append(new_row)#, ignore_index = True)\n",
    "\n",
    "skew_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skew_df.columns.name = 'Skewness Values'\n",
    "skew_df.index.name = 'Lab Measures'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The figure below shows the skewness values for each lab measurement for normalized data that has been transformed by taking the log, the square root and using the box-cox function. It can be seen that the box-cox function performed better than other transforms in reducing skewness. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = skew_df.plot.bar(figsize = (20,7), title = 'Skewness Values for Different Data Transforms')\n",
    "ax.set_ylabel(\"Skewness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sqrt_norm_data = np.sqrt(data2_norm[data2_norm.columns[1:]])\n",
    "#log_norm_data = np.log(data2_norm[data2_norm.columns[1:]].add(1))\n",
    "boxcox_data = data2.copy()\n",
    "for feature_name in data2.columns[1:]:\n",
    "    boxcox_data[feature_name] = scats.boxcox(data2[feature_name].add(1))[0]\n",
    "\n",
    "boxcox_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to identify outliers in normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_dict = {}\n",
    "suffix = '_outliers'\n",
    "\n",
    "\n",
    "for feature in boxcox_data.cols():\n",
    "    if feature != 'mort_icu':\n",
    "        # TODO: Calculate Q1 (25th percentile of the data) for the given feature\n",
    "        Q1 = np.percentile(boxcox_data[feature], 25)\n",
    "\n",
    "        # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n",
    "        Q3 = np.percentile(boxcox_data[feature], 75)\n",
    "\n",
    "        # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "        step = 1.5*(Q3 - Q1)\n",
    "\n",
    "        # Display the outliers\n",
    "        # print \"Data points considered outliers for the feature '{}':\".format(feature)\n",
    "        # display(data2[~((data2[feature] >= Q1 - step) & (data2[feature] <= Q3 + step))])\n",
    "        names_dict[feature+suffix] = boxcox_data[~((boxcox_data[feature] >= Q1 - step) & (boxcox_data[feature] <= Q3 + step))].index\n",
    "# OPTIONAL: Select the indices for data points you wish to remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to remove datapoints from normalized data with 3 or more variables that are outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_outliers = []\n",
    "for feature in names_dict.cols():\n",
    "    all_outliers.extend(names_dict[feature])\n",
    "print \"the total outlier indices = {}\".format(len(all_outliers))\n",
    "indices, counts = np.unique(all_outliers, return_counts = True)\n",
    "outlier_dict = {'counts': counts,\n",
    "                'indices': indices\n",
    "               }\n",
    "outlier_df = pd.DataFrame(outlier_dict)\n",
    "\n",
    "outliers  = list(outlier_df[outlier_df.counts >= 2].indices.values)\n",
    "print \"the following data points have >2 outlying feature and will be removed: \\n{}\".format(outliers)\n",
    "# Remove the outliers, if any were specified\n",
    "\n",
    "\n",
    "boxcox_data2 = boxcox_data.drop(boxcox_data.index[outliers]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# I believe this can be done more efficiently with sklearn.preprocessing.scale\n",
    "boxcox_data_scaled = boxcox_data2.copy()\n",
    "for feature_name in boxcox_data2.columns[1:]:\n",
    "#    max_value = data2[feature_name].max()\n",
    "#    min_value = data2[feature_name].min()\n",
    "#    data2_norm[feature_name] = (data2[feature_name] - min_value) / (max_value - min_value)\n",
    "    boxcox_data_scaled[feature_name] = preprocessing.scale(boxcox_data_scaled[feature_name], with_mean = True, \n",
    "                                                  with_std = True)\n",
    "    \n",
    "display(boxcox_data_scaled.head())\n",
    "display(boxcox_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# a dictionary is created containing units of measure for the different lab measurements\n",
    "labs_dict2 = labs_dict\n",
    "\n",
    "# could be done more succinctly using the dictionary described above\n",
    "\n",
    "for item in remove_list:\n",
    "    del labs_dict2[item]\n",
    "\n",
    "print labs_dict2\n",
    "print boxcox_data_scaled.columns\n",
    "\n",
    "import collections\n",
    "labs_dict3 = collections.OrderedDict(sorted(labs_dict2.items()))\n",
    "labs_dict3\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting frequency distribution for each lab measurement for both survival and non-survival groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for l, u in iter(sorted(labs_dict.iteritems())):\n",
    "    # count patients\n",
    "    n_nonsurv = boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==1].dropna().count()\n",
    "    n_surv = boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==0].dropna().count()\n",
    "    \n",
    "    # get median, variance, skewness\n",
    "    med_nonsurv = boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==1].dropna().median()\n",
    "    med_surv = boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==0].dropna().median()\n",
    "    var_nonsurv = boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==1].dropna().var()\n",
    "    var_surv = boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==0].dropna().var()\n",
    "    skew_nonsurv = boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==1].dropna().skew()\n",
    "    skew_surv = boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==0].dropna().skew() \n",
    "    \n",
    "    # Are the 2 samples drawn from the same continuous distribution? \n",
    "    # Try Kolmogorov Smirnov test \n",
    "    ks_stat, p_val = ks_2samp(boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==1].dropna(),\n",
    "                              boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==0].dropna())\n",
    "\n",
    "    # plot\n",
    "    plt.subplots(figsize=(13,6))\n",
    "    boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==1].dropna().plot.kde(\n",
    "        alpha=1.0,label='Non-survival (n={})'.format(n_nonsurv))\n",
    "    boxcox_data_scaled[l][boxcox_data_scaled.mort_icu==0].dropna().plot.kde(\n",
    "        alpha=1.0,label='Survival (n={})'.format(n_surv))\n",
    "    \n",
    "    # fake plots for KS test, median, etc\n",
    "    plt.plot([], label=' ',color='lightgray')\n",
    "    plt.plot([], label='KS test: p={}'.format(format(p_val,'.3f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Median (non-surv): {}'.format(format(med_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Median (surv): {}'.format(format(med_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Variance (non-surv): {}'.format(format(var_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Variance (surv): {}'.format(format(var_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Skew (non-surv): {}'.format(format(skew_nonsurv,'.2f')),\n",
    "             color='lightgray')\n",
    "    plt.plot([], label='Skew (surv): {}'.format(format(skew_surv,'.2f')),\n",
    "             color='lightgray')\n",
    "\n",
    "    # add title, labels etc.\n",
    "    plt.title('First {} measurement on ICU admission '.format(l.lower()) +\n",
    "               'vs ICU mortality \\n')\n",
    "    plt.xlabel(l + ' ' + u)\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1),fontsize=12)\n",
    "    #plt.xlim(0, boxcox_data_scaled[l].dropna().quantile(0.99))\n",
    "    \n",
    "    # Add lab range if available\n",
    "    if l in lab_ranges:\n",
    "        plt.axvline(lab_ranges[l][0],color='k',linestyle='--')\n",
    "        plt.axvline(lab_ranges[l][1],color='k',linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing PCA on box-cox transformed, scaled data with outliers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# principle component analysis is used to reduce the dimensionality of data\n",
    "\n",
    "#pca = PCA(n_components = 4).fit(data2)\n",
    "pca = PCA(n_components = 4).fit(boxcox_data_scaled[boxcox_data_scaled.columns[1:]])\n",
    "\n",
    "# Generate PCA results plot\n",
    "pca_results = vs.pca_results(boxcox_data_scaled[boxcox_data_scaled.columns[1:]], pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduced_data = pca.transform(boxcox_data_scaled[boxcox_data_scaled.columns[1:]])\n",
    "# Create a DataFrame for the reduced data\n",
    "reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2', 'Dimension 3', 'Dimension 4']) #,\n",
    "                                                    #'Dimension 5', 'Dimension 6','Dimension 7', 'Dimension 8'])\n",
    "reduced_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot of Box-Cox transformed data with outliers removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.scatter_matrix(boxcox_data_scaled[1:], alpha = 0.3, figsize = (14,8), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized data principle components split into training and testing groups. A Gaussian Naive Bayes classifier is trained and tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training using 80% of data and testing using 20%. \n",
    "X_train, X_test, y_train, y_test = train_test_split(reduced_data, \n",
    "                                                    boxcox_data_scaled['mort_icu'], test_size = .30, random_state = 42)\n",
    "\n",
    "\n",
    "### create classifier\n",
    "clf_GNB = GaussianNB()\n",
    "### fit the classifier on the training features and labels\n",
    "clf_GNB.fit(X_train, y_train)\n",
    "    ### return the fit classifier\n",
    "\n",
    "nb_score = clf_GNB.score(X_test, y_test) \n",
    "\n",
    "print nb_score\n",
    "y_predsGNB = clf_GNB.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_predsGNB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box-Cox transformed data principle components split into training and testing groups. A Gaussian Naive Bayes classifier is trained and tested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classifier was trained and tested using principle components of normalized data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(boxcox_data_scaled[boxcox_data_scaled.columns[1:]], \n",
    "                                                    boxcox_data_scaled['mort_icu'], test_size = 0.40, random_state = 42)\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-6, learning_rate = 'adaptive',\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)                         \n",
    "\n",
    "display(clf.score(X_test, y_test))\n",
    "y_predsMLP = clf.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_predsMLP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[data.columns[5:]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classifier was trained and tested using principle components of normalized data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxcox_dead = boxcox_data_scaled[boxcox_data_scaled.mort_icu == 1]\n",
    "boxcox_survivors = boxcox_data_scaled[boxcox_data_scaled.mort_icu == 0]\n",
    "display(boxcox_dead.shape[0])\n",
    "display(boxcox_survivors.shape[0])\n",
    "boxcox_survivors_reduced = boxcox_survivors.sample(boxcox_dead.shape[0])\n",
    "frames = [boxcox_survivors_reduced, boxcox_dead]\n",
    "boxcox_even = pd.concat(frames)\n",
    "boxcox_even.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_samps = boxcox_even.shape[0]\n",
    "display(num_samps)\n",
    "\n",
    "boxcox_even2 = boxcox_even.sample(n=num_samps)\n",
    "#boxcox_even2.shape()\n",
    "boxcox_even2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxcox_even2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training using 80% of data and testing using 20%. \n",
    "X_train, X_test, y_train, y_test = train_test_split(boxcox_even2[boxcox_even2.columns[1:]], \n",
    "                                                    boxcox_even['mort_icu'], test_size = 0.40, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "### create classifier\n",
    "clf_GNB = GaussianNB()\n",
    "### fit the classifier on the training features and labels\n",
    "clf_GNB.fit(X_train, y_train)\n",
    "    ### return the fit classifier\n",
    "\n",
    "nb_score = clf_GNB.score(X_test, y_test) \n",
    "\n",
    "print nb_score\n",
    "y_predsGNB = clf_GNB.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_predsGNB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Classification on un-processed data with variables selected based on visual inspection\n",
    "The variables enumerated in 'remove_list' were observed for both survivors and non-survivors and appeared to be very similar in distribution. Because they appeared to be very similar for both groups, they were excluded as features for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(boxcox_even2[boxcox_even2.columns[1:]], \n",
    "                                                    boxcox_even['mort_icu'], test_size = 0.30, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "\n",
    "\n",
    "clf_SVC = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf', \n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False).fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(clf_SVC, boxcox_data_scaled[boxcox_data_scaled.columns[1:]], \n",
    "                         boxcox_data_scaled['mort_icu'], cv=5)\n",
    "display(scores)  \n",
    "y_predsSVC = clf_SVC.predict(X_test)\n",
    "display(metrics.confusion_matrix(y_test, y_predsSVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(clf_SVC.score(X_test, y_test))\n",
    "y_predsSVC = clf_SVC.predict(X_test)\n",
    "display(metrics.confusion_matrix(y_test, y_predsSVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(boxcox_even2[boxcox_even2.columns[1:]], \n",
    "                                                    boxcox_even['mort_icu'], test_size = 0.30, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-6, learning_rate = 'adaptive',\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)                         \n",
    "\n",
    "display(clf.score(X_test, y_test))\n",
    "y_predsMLP = clf.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_predsMLP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"moncol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diagnoses_codes = pd.DataFrame.from_csv('IDC9_DEADLY_DIAGNOSES.csv')\n",
    "diagnoses_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diagnoses_list = diagnoses_codes.icd9_code.unique()\n",
    "diagnoses_list2 = diagnoses_codes.diagnosis.unique()\n",
    "display(len(diagnoses_list))\n",
    "display(len(diagnoses_list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_codes = diagnoses_codes.drop_duplicates(['icd9_code', 'short_title'])\n",
    "unique_codes.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(diagnoses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diag_counts = diagnoses_codes.icd9_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diag_counts[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diagnoses_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxcox_data_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxcox_data_scaled_subset = boxcox_data_scaled[['mort_icu', 'albumin_1st', 'bicarbonate_1st', 'inr_1st', 'phosphate_1st',\n",
    "                                                'ptt_1st']]\n",
    "boxcox_data_scaled_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# principle component analysis is used to reduce the dimensionality of data\n",
    "\n",
    "#pca = PCA(n_components = 4).fit(data2)\n",
    "pca = PCA(n_components = 3).fit(boxcox_data_scaled[boxcox_data_scaled.columns[1:]])\n",
    "\n",
    "# Generate PCA results plot\n",
    "pca_results = vs.pca_results(boxcox_data_scaled[boxcox_data_scaled.columns[1:]], pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduced_data = pca.transform(boxcox_data_scaled[boxcox_data_scaled.columns[1:]])\n",
    "# Create a DataFrame for the reduced data\n",
    "reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2', 'Dimension 3', 'Dimension 4']) #,\n",
    "                                                    #'Dimension 5', 'Dimension 6','Dimension 7', 'Dimension 8'])\n",
    "reduced_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot of Box-Cox transformed data with outliers removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized data principle components split into training and testing groups. A Gaussian Naive Bayes classifier is trained and tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_list = list(boxcox_data_scaled_subset.columns[1:])\n",
    "feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training using 80% of data and testing using 20%. \n",
    "X_train, X_test, y_train, y_test = train_test_split(boxcox_data_scaled_subset[feat_list], \n",
    "                                                    boxcox_data_scaled_subset['mort_icu'], \n",
    "                                                    test_size = .30, random_state = 42)\n",
    "\n",
    "\n",
    "### create classifier\n",
    "clf_GNB = GaussianNB()\n",
    "### fit the classifier on the training features and labels\n",
    "clf_GNB.fit(X_train, y_train)\n",
    "    ### return the fit classifier\n",
    "\n",
    "nb_score = clf_GNB.score(X_test, y_test) \n",
    "\n",
    "print nb_score\n",
    "y_predsGNB = clf_GNB.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_predsGNB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
